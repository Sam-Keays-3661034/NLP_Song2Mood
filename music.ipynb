{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e0dff1b",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8901e76",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sultanarazia/miniconda3/envs/nlp_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "import gensim.downloader\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8434a0e5",
   "metadata": {},
   "source": [
    "## Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5e8a198-92c0-4eb3-bc62-435c35a70434",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_tokenize_pattern = re.compile(r\"(?u)\\b\\w\\w+\\b\")\n",
    "\n",
    "\n",
    "def word_tokenize(s: str):\n",
    "    return [x.lower() for x in word_tokenize_pattern.findall(s)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e16e61b3",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb3292a6-f7ab-4858-9732-76ba312fe93d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "\n",
    "EMOTIONS = [\"Angry\", \"Happy\", \"Relaxed\", \"Sad\"]\n",
    "\n",
    "def print_results(gold_labels, predicted_labels):\n",
    "    # overall\n",
    "    p, r, f, _ = precision_recall_fscore_support(\n",
    "        gold_labels, predicted_labels, average=\"macro\", zero_division=0\n",
    "    )\n",
    "    acc = accuracy_score(gold_labels, predicted_labels)\n",
    "\n",
    "    print(\"=== Overall (Macro Avg) ===\")\n",
    "    print(\"Precision:\", p)\n",
    "    print(\"Recall:\", r)\n",
    "    print(\"F1:\", f)\n",
    "    print(\"Accuracy:\", acc)\n",
    "    print()\n",
    "\n",
    "    # Per-emotion metrics\n",
    "    p_i, r_i, f_i, _ = precision_recall_fscore_support(\n",
    "        gold_labels, predicted_labels, average=None, zero_division=0\n",
    "    )\n",
    "\n",
    "    print(\"=== Per Emotion (Class) Metrics ===\")\n",
    "    for i, emotion in enumerate(EMOTIONS):\n",
    "        print(f\"{emotion}:\")\n",
    "        print(\"  Precision:\", p_i[i])\n",
    "        print(\"  Recall:   \", r_i[i])\n",
    "        print(\"  F1:       \", f_i[i])\n",
    "    print()\n",
    "\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = logits.argmax(axis=-1)\n",
    "\n",
    "    p, r, f, _ = precision_recall_fscore_support(\n",
    "        labels, preds, average=\"macro\", zero_division=0\n",
    "    )\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\"precision\": p, \"recall\": r, \"f1\": f, \"accuracy\": acc}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da6b4b6e",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4edfab4b-46e2-464a-a47f-a2bb86f62c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_DIR = \"NJU_MusicMood_v1.0\"\n",
    "\n",
    "label2id = {label: i for i, label in enumerate(EMOTIONS)}\n",
    "id2label = {i: label for label, i in label2id.items()}\n",
    "\n",
    "timestamp_pattern = re.compile(r\"\\[\\d{2}:\\d{2}(?:\\.\\d{2})?\\]\")\n",
    "\n",
    "\n",
    "def clean_lyrics(text: str) -> str:\n",
    "    # 1. Remove timestamps\n",
    "    text = timestamp_pattern.sub(\"\", text)\n",
    "\n",
    "    # 2. Lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # 3. Normalize quotes\n",
    "    text = text.replace(\"’\", \"'\").replace(\"“\", '\"').replace(\"”\", '\"')\n",
    "\n",
    "    # 4. Strip whitespace from each line\n",
    "    lines = [line.strip() for line in text.splitlines()]\n",
    "\n",
    "    # 5. Remove empty lines\n",
    "    lines = [line for line in lines if line]\n",
    "\n",
    "    # 6. Collapse extra spaces inside lines\n",
    "    lines = [re.sub(r\"\\s+\", \" \", line) for line in lines]\n",
    "\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "\n",
    "def get_lyrics(path: str) -> str:\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        raw = f.read()\n",
    "    return clean_lyrics(raw)\n",
    "\n",
    "\n",
    "def get_lyrics_and_labels(split: str):\n",
    "    texts, labels = [], []\n",
    "    for emotion in EMOTIONS:\n",
    "        folder = os.path.join(DATASET_DIR, emotion, split)\n",
    "        if not os.path.isdir(folder):\n",
    "            continue\n",
    "\n",
    "        for fname in os.listdir(folder):\n",
    "            if not fname.endswith(\".txt\"):\n",
    "                continue\n",
    "            if fname.lower() == \"info.txt\":\n",
    "                continue\n",
    "\n",
    "            path = os.path.join(folder, fname)\n",
    "            txt = get_lyrics(path)\n",
    "            if txt.strip():\n",
    "                texts.append(txt)\n",
    "                labels.append(emotion)\n",
    "    return texts, labels\n",
    "\n",
    "\n",
    "# Load data \n",
    "train_texts, train_labels = get_lyrics_and_labels(\"Train\")\n",
    "dev_texts, dev_labels = get_lyrics_and_labels(\"Test\")\n",
    "\n",
    "assert len(train_texts) == len(train_labels)\n",
    "assert len(dev_texts) == len(dev_labels)\n",
    "\n",
    "# Datasets\n",
    "train_ds = Dataset.from_dict(\n",
    "    {\"text\": train_texts, \"label\": [label2id[l] for l in train_labels]}\n",
    ")\n",
    "dev_ds = Dataset.from_dict(\n",
    "    {\"text\": dev_texts, \"label\": [label2id[l] for l in dev_labels]}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f93414e",
   "metadata": {},
   "source": [
    "## Baseline: Bag of Words & Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f5a40396-55f3-43dc-af8e-fd45ad82c685",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Baseline: Bag of Words & Logistic Regression ===\n",
      "=== Overall (Macro Avg) ===\n",
      "Precision: 0.37987563509974537\n",
      "Recall: 0.37369068092033764\n",
      "F1: 0.37382782926716535\n",
      "Accuracy: 0.363395225464191\n",
      "\n",
      "=== Per Emotion (Class) Metrics ===\n",
      "Angry:\n",
      "  Precision: 0.5\n",
      "  Recall:    0.5070422535211268\n",
      "  F1:        0.5034965034965035\n",
      "Happy:\n",
      "  Precision: 0.45121951219512196\n",
      "  Recall:    0.3490566037735849\n",
      "  F1:        0.39361702127659576\n",
      "Relaxed:\n",
      "  Precision: 0.3023255813953488\n",
      "  Recall:    0.38613861386138615\n",
      "  F1:        0.3391304347826087\n",
      "Sad:\n",
      "  Precision: 0.26595744680851063\n",
      "  Recall:    0.25252525252525254\n",
      "  F1:        0.25906735751295334\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=== Baseline: Bag of Words & Logistic Regression ===\")\n",
    "\n",
    "count_vectorizer = CountVectorizer(analyzer=word_tokenize, max_features=30000 )\n",
    "train_counts = count_vectorizer.fit_transform(train_texts)\n",
    "dev_counts = count_vectorizer.transform(dev_texts)\n",
    "\n",
    "lr_bow = LogisticRegression(max_iter=2000,\n",
    "    class_weight=\"balanced\",\n",
    "    solver=\"lbfgs\",\n",
    "    random_state=0)\n",
    "lr_bow.fit(train_counts, train_labels)\n",
    "\n",
    "lr_bow_dev_predictions = lr_bow.predict(dev_counts)\n",
    "print_results(dev_labels, lr_bow_dev_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "919b2d22-6478-41ca-9e80-f48ab42df9c7",
   "metadata": {},
   "source": [
    "## Baseline: Tf-IDF & Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1172eecd-9c37-4301-ade5-1159d66de438",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training TF-IDF Logistic Regression model...\n",
      "=== Overall (Macro Avg) ===\n",
      "Precision: 0.46542427022911215\n",
      "Recall: 0.4838117016591375\n",
      "F1: 0.47157415532186603\n",
      "Accuracy: 0.46419098143236076\n",
      "\n",
      "=== Per Emotion (Class) Metrics ===\n",
      "Angry:\n",
      "  Precision: 0.6309523809523809\n",
      "  Recall:    0.7464788732394366\n",
      "  F1:        0.6838709677419355\n",
      "Happy:\n",
      "  Precision: 0.4628099173553719\n",
      "  Recall:    0.5283018867924528\n",
      "  F1:        0.4933920704845815\n",
      "Relaxed:\n",
      "  Precision: 0.3875\n",
      "  Recall:    0.3069306930693069\n",
      "  F1:        0.3425414364640884\n",
      "Sad:\n",
      "  Precision: 0.3804347826086957\n",
      "  Recall:    0.35353535353535354\n",
      "  F1:        0.36649214659685864\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sultanarazia/miniconda3/envs/nlp_env/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:533: UserWarning: The parameter 'ngram_range' will not be used since 'analyzer' is callable'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    analyzer=word_tokenize,\n",
    "    max_features=30000,        \n",
    "    ngram_range=(1, 2),  #unigram and bigram     \n",
    "    sublinear_tf=True,   # log-scaled term frequency \n",
    ")\n",
    "\n",
    "# Fit TF-IDF on training text\n",
    "train_tfidf = tfidf_vectorizer.fit_transform(train_texts)\n",
    "dev_tfidf = tfidf_vectorizer.transform(dev_texts)\n",
    "\n",
    "# Logistic Regression classifier\n",
    "lr_tfidf = LogisticRegression(\n",
    "    max_iter=2000,\n",
    "    class_weight=\"balanced\",\n",
    "    # multi_class=\"multinomial\",\n",
    "    solver=\"lbfgs\",\n",
    "    random_state=0\n",
    ")\n",
    "\n",
    "print(\"Training TF-IDF Logistic Regression model...\")\n",
    "lr_tfidf.fit(train_tfidf, train_labels)\n",
    "\n",
    "# Predict on the dev set\n",
    "lr_tfidf_predictions = lr_tfidf.predict(dev_tfidf)\n",
    "\n",
    "# Evaluate performance\n",
    "print_results(dev_labels, lr_tfidf_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80bab194",
   "metadata": {},
   "source": [
    "## Baseline 2: Word2Vec & Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e417a9d1-ef6c-4527-a6dc-49d0e591d404",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Word2Vec & Logistic Regression ===\n",
      "=== Overall (Macro Avg) ===\n",
      "Precision: 0.4596175291565\n",
      "Recall: 0.47568620468212114\n",
      "F1: 0.45246239781207337\n",
      "Accuracy: 0.4509283819628647\n",
      "\n",
      "=== Per Emotion (Class) Metrics ===\n",
      "Angry:\n",
      "  Precision: 0.6063829787234043\n",
      "  Recall:    0.8028169014084507\n",
      "  F1:        0.6909090909090909\n",
      "Happy:\n",
      "  Precision: 0.47572815533980584\n",
      "  Recall:    0.46226415094339623\n",
      "  F1:        0.4688995215311005\n",
      "Relaxed:\n",
      "  Precision: 0.3308270676691729\n",
      "  Recall:    0.43564356435643564\n",
      "  F1:        0.37606837606837606\n",
      "Sad:\n",
      "  Precision: 0.425531914893617\n",
      "  Recall:    0.20202020202020202\n",
      "  F1:        0.273972602739726\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=== Word2Vec & Logistic Regression ===\")\n",
    "\n",
    "w2v_model = gensim.downloader.load(\"word2vec-google-news-300\")\n",
    "VECTOR_SIZE = w2v_model.vector_size\n",
    "\n",
    "\n",
    "def vec_for_doc(tokenized_doc):\n",
    "    vectors = [w2v_model[word] for word in tokenized_doc if word in w2v_model.key_to_index]\n",
    "    if not vectors:\n",
    "        return np.zeros(VECTOR_SIZE, dtype=\"float32\")\n",
    "    return np.mean(vectors, axis=0)\n",
    "\n",
    "\n",
    "train_vecs = [vec_for_doc(word_tokenize(x)) for x in train_texts]\n",
    "dev_vecs = [vec_for_doc(word_tokenize(x)) for x in dev_texts]\n",
    "\n",
    "lr_w2v = LogisticRegression(max_iter=500, random_state=0)\n",
    "lr_w2v.fit(train_vecs, train_labels)\n",
    "\n",
    "w2v_dev_predictions = lr_w2v.predict(dev_vecs)\n",
    "print_results(dev_labels, w2v_dev_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5280b0e-da94-465e-8776-7447e46db1e8",
   "metadata": {},
   "source": [
    "## Most Frequent Class Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6d8e5bb7-4e1b-463e-8601-a9d02ab72748",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Baseline: Most Frequent Class (MFC) ===\n",
      "Most frequent class id: 0\n",
      "=== Overall (Macro Avg) ===\n",
      "Precision: 0.047082228116710874\n",
      "Recall: 0.25\n",
      "F1: 0.07924107142857142\n",
      "Accuracy: 0.1883289124668435\n",
      "\n",
      "=== Per Emotion (Class) Metrics ===\n",
      "Angry:\n",
      "  Precision: 0.1883289124668435\n",
      "  Recall:    1.0\n",
      "  F1:        0.3169642857142857\n",
      "Happy:\n",
      "  Precision: 0.0\n",
      "  Recall:    0.0\n",
      "  F1:        0.0\n",
      "Relaxed:\n",
      "  Precision: 0.0\n",
      "  Recall:    0.0\n",
      "  F1:        0.0\n",
      "Sad:\n",
      "  Precision: 0.0\n",
      "  Recall:    0.0\n",
      "  F1:        0.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "print(\"=== Baseline: Most Frequent Class (MFC) ===\")\n",
    "\n",
    "train_labels = train_ds[\"label\"]\n",
    "dev_labels = dev_ds[\"label\"]\n",
    "\n",
    "# Count frequency of each label\n",
    "label_counts = Counter(train_labels)\n",
    "most_frequent_label = label_counts.most_common(1)[0][0]\n",
    "\n",
    "print(\"Most frequent class id:\", most_frequent_label)\n",
    "\n",
    "# Predict this label for all dev samples\n",
    "mfc_predictions = [most_frequent_label] * len(dev_labels)\n",
    "\n",
    "# Evaluate\n",
    "print_results(dev_labels, mfc_predictions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9969b75d-5953-4c6a-b807-3c00fb4a940e",
   "metadata": {},
   "source": [
    "## Benchmark Model Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8275c4c9-0c1f-4969-a64a-e055131ce792",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_dataset(dataset, tokenizer, max_length: int = 256):\n",
    "    def _tok(batch):\n",
    "        return tokenizer(\n",
    "            batch[\"text\"],\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=max_length,\n",
    "        )\n",
    "\n",
    "    tokenized = dataset.map(_tok, batched=True)\n",
    "    tokenized = tokenized.remove_columns([\"text\"])\n",
    "    tokenized.set_format(type=\"torch\")\n",
    "    return tokenized\n",
    "\n",
    "\n",
    "def train_and_eval_transformer(\n",
    "    model_name: str,\n",
    "    train_dataset: Dataset,\n",
    "    dev_dataset: Dataset,\n",
    "    output_dir: str,\n",
    "    num_epochs: int,\n",
    "    learning_rate: float,\n",
    "    train_bs: int,\n",
    "    eval_bs: int,\n",
    "    set_pad_token_eos: bool = False,\n",
    "):\n",
    "    print(f\"=== Fine-tuning {model_name} ===\")\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    if set_pad_token_eos:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    tokenized_train = tokenize_dataset(train_dataset, tokenizer)\n",
    "    tokenized_dev = tokenize_dataset(dev_dataset, tokenizer)\n",
    "\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_name,\n",
    "        num_labels=len(EMOTIONS),\n",
    "        label2id=label2id,\n",
    "        id2label=id2label,\n",
    "        ignore_mismatched_sizes=True,\n",
    "    )\n",
    "\n",
    "    if set_pad_token_eos:\n",
    "        model.config.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        num_train_epochs=num_epochs,\n",
    "        learning_rate=learning_rate,\n",
    "        per_device_train_batch_size=train_bs,\n",
    "        per_device_eval_batch_size=eval_bs,\n",
    "        weight_decay=0.01,\n",
    "        eval_strategy=\"epoch\",\n",
    "        logging_steps=16,\n",
    "        log_level=\"error\",\n",
    "        report_to=\"none\",\n",
    "        save_strategy=\"epoch\",\n",
    "        dataloader_pin_memory=False,\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_train,\n",
    "        eval_dataset=tokenized_dev,\n",
    "        compute_metrics=compute_metrics,\n",
    "        tokenizer=tokenizer,\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "    eval_results = trainer.evaluate()\n",
    "    print(f\"{model_name} dev results:\", eval_results)\n",
    "\n",
    "    pred_output = trainer.predict(tokenized_dev)\n",
    "    logits = pred_output.predictions\n",
    "    pred_ids = np.argmax(logits, axis=-1)\n",
    "    pred_labels = [id2label[i] for i in pred_ids]\n",
    "\n",
    "    print(f\"{model_name} classification report:\")\n",
    "    print_results(dev_labels, pred_labels)\n",
    "\n",
    "    return trainer, eval_results, pred_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f02eb96",
   "metadata": {},
   "source": [
    "## DistilGPT2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c13716d5-8f59-4ddc-b52f-32e87a2d3c8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'what′s the difference of never knowing at all?\\nwhen every step i take is always too small.\\nmaybe it′s just something i can′t admit but lately,\\ni feel like i don′t give a shit.\\nmotivation such an aggravation,\\naccusations don′t know how to take them.\\ninspiration′s getting hard to fake it.\\nconcentration′s never hard to brake it.\\nsituation never what you want it to be.\\nwhat′s the point of never making mistakes?\\nself-indulgence is such a hard habit to brake.\\nit′s all just a waste of time in the end.\\ni don′t care so why should i even pretend.\\nmotivation such an aggravation,\\naccusations don′t know how to take them.\\ninspiration′s getting hard to fake it.\\nconcentration′s never hard to brake it.\\nsituation never what you want it.\\nnothing′s new, everything′s the same.\\nit keeps on dragging me down, it′s getting kind of lame.\\ni′m falling further behind, there′s nothing to explain.\\nno matter what you say nothing ′s gonna change my mind.\\ncan′t pretend on doubt until the end.\\nit seems like leaving friends has become\\nthis years trend and though i can′t pretend.\\nit′s not the same but who′s to blame,\\nor all those stupid things i never said.\\nmotivation such an aggravation,\\naccusations don′t know how to take them.\\ninspiration′s getting hard to fake it.\\nconcentration′s never hard to brake it.\\nmotivation such an aggravation,\\naccusations don′t know how to take them.\\ninspiration′s getting hard to fake it.\\nconcentration′s never hard to brake it.\\nsituation never what you want it to be.\\nnever what you want it to be.\\nnever what you want it to be.', 'label': 0}\n",
      "--------------\n",
      "{'text': 'i\\'m going through changes\\ni\\'m going through changes\\nlately i really, feel like i\\'m rolling for ___ like philly,\\ni feel like i\\'m losing control of myself, i sincerely,\\ni apologize if all that i sound like, is i\\'m complaining,\\nbut life keeps on complicating, an\\' i\\'m debating,\\non leaving this world, this evening, even my girls,\\ncan see i\\'m grievin\\', i try and hide it,\\nbut i can\\'t, why do i act like i\\'m all high and mighty,\\nwhen inside, i\\'m dying, i am finally realizing i need help.\\ni can\\'t do it by myself, too weak, 2 weeks i\\'ve been having ups and downs,\\ngoing through peaks and valleys, dilly dallying,\\naround with the idea, of ending the shit right here.\\ni\\'m hatin\\' my reflection, i walk around the house tryin\\' to fight mirrors,\\ni can\\'t stand what i look like, yeah, i look fat, but what do i care?\\ni give a fuck, only thing i fear, is hailie,\\ni\\'m afraid if i close my eyes i might see her,\\nshit..\\ni\\'m going through changes\\ni\\'m going through changes\\ni lock myself in the bedroom, bathroom, nappin\\' at noon,\\nyeah dad\\'s in a bad mood, he\\'s always snappin\\' at you.\\nmarshall what happened at you, you can\\'t stop with these pills,\\nand you\\'ve fallen off with your skills, and your own fans are laughin\\' at you.\\nit become a problem you\\'re too pussy to tackle, get up,\\nbe a man, stand, a real man woulda had this shit handled.\\nknow you just had your heart ripped out and crushed,\\nthey say proof just flipped out, homie just swift out and bust,\\nnah, it ain\\'t like doody to do that,\\nhe wouldn\\'t fuckin\\' shoot at, no-body, he fights first,\\nbut dwellin\\' on it only makes the night worse,\\nnow i\\'m poppin vic\\'s, perks and methadone pills.\\nyeah em, tight verse, you killed it,\\nfuckin\\' drug dealers hang around me like \"yes man\",\\nand they gon\\' do whatever i says when, i says it,\\nit\\'s in their best interest to protect their investment.\\nand i just lost my fuckin\\' best friend, so fuck it, i guess then...\\ni\\'m going through changes\\ndon\\'t know what i\\'m going through, but i just keep on going through changes...\\nmy friends just can\\'t understand this new me,\\nthat\\'s understandable man, but just think how bananas you\\'d be,\\nyou\\'d be an animal too, if you were trapped in this fame and caged in it like a zoo.\\nand everybody\\'s lookin\\' at you, what you want me to do,\\ni\\'m startin\\' to live like a recluse and the truth is,\\nfame startin\\' to give me an excuse, to be at a all time low.\\ni sit alone in my home theatre, watchin\\' the same damn dvd,\\nof the first tour, the last tour, he was still alive.\\nand it hurt sore, fast forward, sleepin\\' pills\\'ll make me feel alright.\\nand if i\\'m still awake in the middle of the night,\\ni just take a couple more, yeah you\\'re motherfuckin\\' right,\\ni ain\\'t slowin\\' down for no one, i am almost homeward bound.\\nalmost in a coma, yeah homie come on, don\\'t look now,\\ndaddy, don\\'t you die on me, daddy, better hold your ground.\\nfuck, don\\'t i know the sound of that voice,\\nyeah baby hold me down.\\ni\\'m going through changes\\ndon\\'t know what i\\'m going through, but i just keep on going through changes...\\nwake up in the hospital, full of tubes, plus somehow i\\'m pullin\\' through.\\nswear when i come back i\\'ma be bulletproof.\\ni\\'ma do it just for proof, i think i should state a few,\\nfacts, cause i may not get a chance again to say the truth.\\nshit it just hit me that what if i would notta made it through?\\ni think about the things i would never got to say to you,\\ni\\'d never get to make it right, so here\\'s what i came to do.\\nhailie this one is for you, whitney and alaina too,\\ni still love your mother, that\\'ll never change,\\nthink about her every day, we just could never get it together.\\nhey, wish there was a better way, for me to say it,\\nbut i swear on everything, i\\'d do anything for her on anyday.\\nthere are just too many things, to explain, when it rains,\\nguess it pours, yes it does, wish there wasn\\'t any pain.\\nbut i can\\'t pretend there ain\\'t, i ain\\'t placin\\' any blame,\\ni ain\\'t pointin\\' fingers, heaven knows there never been a saint.\\ni know it just feels like we just pissed away our history,\\nbut just today, i looked at your picture, almost hate to say,\\ni miss you self consciously, wish it didn\\'t end this way.\\nbut i just had to get away, don\\'t know why,\\ni don\\'t know what else to say, i guess i\\'m..\\ni\\'m going through changes\\ndon\\'t know what i\\'m going through, but i just keep on going through changes...\\neminem - going through changes\\nend... ...', 'label': 0}\n",
      "--------------\n",
      "{'text': 'i took my lover to the sea\\ndrowned him precious bride to be\\nbleeding heaven from the start\\nwoe is rapture ripped apart\\nhey you\\'re honest, aren\\'t you?\\nshow me rapture torn apart\\neverybody\\'s riddled with disease\\nbut i bet you say, \"not me\"\\nso honest, aren\\'t you?\\nyou\\'re so honest, aren\\'t you so?\\n.\\nmy, aren\\'t you\\nsurely the finest of the brigade?\\nmy, aren\\'t you\\nalways right a portrait of dignity?\\nmy, aren\\'t you\\ninnocent and never suspectable\\nmy, aren\\'t you\\n.\\nwait\\nyou smell like sh*t, not the truth\\nfull of device, not devotion\\nconscience came right up to you\\nand then you threw it back\\nyou are the scum of the earth\\nyou are the scum of the ocean\\nto you it\\'s above as below\\nyou smear your filth across the world\\nyou smear your filth across the world\\nyou smear your filth across the world\\nyou smear your filth across the world\\n.', 'label': 0}\n",
      "--------------\n",
      "{'text': \"i came into this world as a reject\\nlook into these eyes\\nthen you'll see the size of the flames\\ndwellin on the past\\nits burnin up my brain\\neveryone that burns has to learn from the pain\\nhey, i think about the day\\nmy girlie ran away with my pay\\nmy fellas came to play\\nnow shes stuck with my homies that she fucked\\nand i'm just a sucker with a lump in my throat\\nhey, like a chump, hey--7x\\nshould i be feelin bad (no)\\nshould i be fellin good (no)\\nits kinda sad\\ni'm the laughing stock of the neighborhood\\nand you would think that i would be movin on\\nbut i'm a sucka like i said\\nfucked up in the head, not\\nand maybe she just made a mistake\\nand i should give her a break\\nmy heart'll ache, either way\\nhey, what the hell you want me to say\\ni won't lie\\nthat i can't deny\\ni did it all for the nookie, come on\\nthe nookie, come on\\nso you can take the cookie and stick it up your(yea)-3x\\ni did it all for the nookie, come on\\nthe nookie, come on\\nso you can take the cookie and stick it up your (yea)-3x\\nwhy did it take so long\\nwhy did i wait so long, huh\\nto figure it out\\nbut i did it\\nand i'm the only one\\nunderneath the sun who didn't get it\\ni can't believe that could be decieved\\n(but you were)by my so called girl, but in reality\\nhad a hidden agenda\\nshe put my tender heart in a blender\\nand still i surrendered\\nhey,like a chump, hey--7x\\ni did it all for the nookie, come on\\nthe nookie, come on\\nso you can take the cookie and stick it up your(yea)-3x\\ni did it all for the nookie, come on\\nthe nookie, come on\\nso you can take the cookie and stick it up your (yea)-3x\\ni'm only human\\nso for your friends give you their advice\\nthey'll tell you, to just let it go\\nits easier said than done\\ni appreciate it, i do, but\\njust leave me alone, leave me alone\\njust leave me alone\\nnothings gunna change\\nyou can go away\\ni'm just gunna stay here and always be the same\\nain't nothing gunna change\\ncuz you can go away\\nand i'm just gunna stay here and always be the same\\nso you can take the cookie and stick it up your (yea)-3x\", 'label': 0}\n",
      "--------------\n",
      "{'text': 'i\\'m not sure who\\'s fooling who here\\nas i\\'m watching your decay\\nwe both know you could deflate\\na 7 hurricane\\nseems like you and your tribe\\ndecided you\\'d rewrite the law\\nsegregate the mind\\nfrom body and from soul\\nyou give me yours\\ni\\'ll give you mine\\ncause i can look your god\\nright in the eye\\nyou give me yours\\ni\\'ll give you mine\\nyou used to look my god\\nright in the eye\\ni believe in defending\\nin what we once\\nstood for\\nit seems in vogue\\nto be a closet\\nmisogynist homophobe\\na change of course in\\nour direction\\na dash of truth\\nspread thinly\\nlike a flag\\non a popstar\\non a benzodiazapene\\nyou give me yours\\ni\\'ll give you mine\\ncause i can look your god\\nright in the eye\\nyou give me yours\\ni\\'ll give you mine\\nyou used to look my god\\nright in the eye\\noh zion please\\nremove your glove\\nand dispel every trace\\nof his spoken word\\nthat has lodged\\nin my vortex\\ni\\'m not sure who\\'s fooling who here\\nas i\\'m watching your decay\\nwe both know you could deflate\\na 7 hurricane\\nyou could have spared\\nher - oh but no\\nmessiahs need\\npeople dying in their name\\nyou could have spared\\nher - oh but no\\nmessiahs need\\npeople dying in their name\\nyou say \"i ordered you a pancake\"\\nyou say \"i ordered you a pancake\"', 'label': 0}\n",
      "--------------\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(train_ds[i])\n",
    "    print(\"--------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dc595940-367c-40d2-96c0-8cc36c91b056",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Fine-tuning distilgpt2 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 400/400 [00:00<00:00, 5305.17 examples/s]\n",
      "Map: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 377/377 [00:00<00:00, 7706.77 examples/s]\n",
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at distilgpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/var/folders/1g/jbw_w_k56sn2h2t6h74q6mlw0000gn/T/ipykernel_58934/2135660195.py:63: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [500/500 04:42, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.491700</td>\n",
       "      <td>1.449112</td>\n",
       "      <td>0.464770</td>\n",
       "      <td>0.261909</td>\n",
       "      <td>0.103441</td>\n",
       "      <td>0.201592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.414400</td>\n",
       "      <td>1.386220</td>\n",
       "      <td>0.405859</td>\n",
       "      <td>0.286774</td>\n",
       "      <td>0.191997</td>\n",
       "      <td>0.307692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.042600</td>\n",
       "      <td>1.264766</td>\n",
       "      <td>0.468613</td>\n",
       "      <td>0.477137</td>\n",
       "      <td>0.456297</td>\n",
       "      <td>0.450928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.561000</td>\n",
       "      <td>1.643822</td>\n",
       "      <td>0.508392</td>\n",
       "      <td>0.524574</td>\n",
       "      <td>0.495061</td>\n",
       "      <td>0.501326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.586800</td>\n",
       "      <td>1.638166</td>\n",
       "      <td>0.504491</td>\n",
       "      <td>0.506339</td>\n",
       "      <td>0.504548</td>\n",
       "      <td>0.488064</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distilgpt2 dev results: {'eval_loss': 1.638166069984436, 'eval_precision': 0.5044906825430081, 'eval_recall': 0.50633895438441, 'eval_f1': 0.5045481885676589, 'eval_accuracy': 0.4880636604774536, 'eval_runtime': 12.3601, 'eval_samples_per_second': 30.501, 'eval_steps_per_second': 7.686, 'epoch': 5.0}\n",
      "distilgpt2 classification report:\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Mix of label input types (string and number)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m gpt2_trainer, gpt2_results, gpt2_pred_labels \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_and_eval_transformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdistilgpt2\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_ds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdev_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdev_ds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./distilgpt2_output\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5e-5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_bs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_bs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mset_pad_token_eos\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[12], line 83\u001b[0m, in \u001b[0;36mtrain_and_eval_transformer\u001b[0;34m(model_name, train_dataset, dev_dataset, output_dir, num_epochs, learning_rate, train_bs, eval_bs, set_pad_token_eos)\u001b[0m\n\u001b[1;32m     80\u001b[0m pred_labels \u001b[38;5;241m=\u001b[39m [id2label[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m pred_ids]\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m classification report:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 83\u001b[0m \u001b[43mprint_results\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdev_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpred_labels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m trainer, eval_results, pred_labels\n",
      "Cell \u001b[0;32mIn[6], line 7\u001b[0m, in \u001b[0;36mprint_results\u001b[0;34m(gold_labels, predicted_labels)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mprint_results\u001b[39m(gold_labels, predicted_labels):\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;66;03m# overall\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m     p, r, f, _ \u001b[38;5;241m=\u001b[39m \u001b[43mprecision_recall_fscore_support\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgold_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpredicted_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maverage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmacro\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mzero_division\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m     acc \u001b[38;5;241m=\u001b[39m accuracy_score(gold_labels, predicted_labels)\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=== Overall (Macro Avg) ===\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp_env/lib/python3.10/site-packages/sklearn/utils/_param_validation.py:218\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    213\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m    214\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    215\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    216\u001b[0m         )\n\u001b[1;32m    217\u001b[0m     ):\n\u001b[0;32m--> 218\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    221\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    222\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    223\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    224\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[1;32m    225\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    226\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    227\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[1;32m    228\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp_env/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1996\u001b[0m, in \u001b[0;36mprecision_recall_fscore_support\u001b[0;34m(y_true, y_pred, beta, labels, pos_label, average, warn_for, sample_weight, zero_division)\u001b[0m\n\u001b[1;32m   1827\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Compute precision, recall, F-measure and support for each class.\u001b[39;00m\n\u001b[1;32m   1828\u001b[0m \n\u001b[1;32m   1829\u001b[0m \u001b[38;5;124;03mThe precision is the ratio ``tp / (tp + fp)`` where ``tp`` is the number of\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1993\u001b[0m \u001b[38;5;124;03m array([2, 2, 2]))\u001b[39;00m\n\u001b[1;32m   1994\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1995\u001b[0m _check_zero_division(zero_division)\n\u001b[0;32m-> 1996\u001b[0m labels \u001b[38;5;241m=\u001b[39m \u001b[43m_check_set_wise_labels\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maverage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpos_label\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1998\u001b[0m \u001b[38;5;66;03m# Calculate tp_sum, pred_sum, true_sum ###\u001b[39;00m\n\u001b[1;32m   1999\u001b[0m samplewise \u001b[38;5;241m=\u001b[39m average \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msamples\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp_env/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1765\u001b[0m, in \u001b[0;36m_check_set_wise_labels\u001b[0;34m(y_true, y_pred, average, labels, pos_label)\u001b[0m\n\u001b[1;32m   1762\u001b[0m y_type, y_true, y_pred \u001b[38;5;241m=\u001b[39m _check_targets(y_true, y_pred)\n\u001b[1;32m   1763\u001b[0m \u001b[38;5;66;03m# Convert to Python primitive type to avoid NumPy type / Python str\u001b[39;00m\n\u001b[1;32m   1764\u001b[0m \u001b[38;5;66;03m# comparison. See https://github.com/numpy/numpy/issues/6784\u001b[39;00m\n\u001b[0;32m-> 1765\u001b[0m present_labels \u001b[38;5;241m=\u001b[39m _tolist(\u001b[43munique_labels\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1766\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m average \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1767\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m y_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp_env/lib/python3.10/site-packages/sklearn/utils/multiclass.py:117\u001b[0m, in \u001b[0;36munique_labels\u001b[0;34m(*ys)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;66;03m# Check that we don't mix string type with number type\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mset\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(label, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m label \u001b[38;5;129;01min\u001b[39;00m ys_labels)) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMix of label input types (string and number)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m xp\u001b[38;5;241m.\u001b[39masarray(\u001b[38;5;28msorted\u001b[39m(ys_labels))\n",
      "\u001b[0;31mValueError\u001b[0m: Mix of label input types (string and number)"
     ]
    }
   ],
   "source": [
    "\n",
    "gpt2_trainer, gpt2_results, gpt2_pred_labels = train_and_eval_transformer(\n",
    "    model_name=\"distilgpt2\",\n",
    "    train_dataset=train_ds,\n",
    "    dev_dataset=dev_ds,\n",
    "    output_dir=\"./distilgpt2_output\",\n",
    "    num_epochs=5,\n",
    "    learning_rate=5e-5,\n",
    "    train_bs=4,\n",
    "    eval_bs=4,\n",
    "    set_pad_token_eos=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "221d4e43",
   "metadata": {},
   "source": [
    "## Distilbert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fe4e592e-e7ad-4d23-b757-580470945074",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Fine-tuning distilbert/distilbert-base-uncased ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 400/400 [00:00<00:00, 4996.31 examples/s]\n",
      "Map: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 377/377 [00:00<00:00, 7422.43 examples/s]\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert/distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/var/folders/1g/jbw_w_k56sn2h2t6h74q6mlw0000gn/T/ipykernel_61759/2135660195.py:63: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='300' max='300' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [300/300 02:13, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.294100</td>\n",
       "      <td>1.287842</td>\n",
       "      <td>0.428011</td>\n",
       "      <td>0.414448</td>\n",
       "      <td>0.336632</td>\n",
       "      <td>0.374005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.809000</td>\n",
       "      <td>1.338264</td>\n",
       "      <td>0.413083</td>\n",
       "      <td>0.485278</td>\n",
       "      <td>0.412546</td>\n",
       "      <td>0.450928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.641800</td>\n",
       "      <td>1.166022</td>\n",
       "      <td>0.557673</td>\n",
       "      <td>0.559182</td>\n",
       "      <td>0.557949</td>\n",
       "      <td>0.541114</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distilbert/distilbert-base-uncased dev results: {'eval_loss': 1.1660218238830566, 'eval_precision': 0.5576731819849255, 'eval_recall': 0.5591815625888924, 'eval_f1': 0.5579492858240676, 'eval_accuracy': 0.5411140583554377, 'eval_runtime': 9.3142, 'eval_samples_per_second': 40.476, 'eval_steps_per_second': 10.199, 'epoch': 3.0}\n",
      "distilbert/distilbert-base-uncased classification report:\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Mix of label input types (string and number)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m distilbert_trainer, distilbert_results, distilbert_pred_labels \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_and_eval_transformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdistilbert/distilbert-base-uncased\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_ds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdev_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdev_ds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./distilbert_musicmood\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5e-5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_bs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_bs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mset_pad_token_eos\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[8], line 83\u001b[0m, in \u001b[0;36mtrain_and_eval_transformer\u001b[0;34m(model_name, train_dataset, dev_dataset, output_dir, num_epochs, learning_rate, train_bs, eval_bs, set_pad_token_eos)\u001b[0m\n\u001b[1;32m     80\u001b[0m pred_labels \u001b[38;5;241m=\u001b[39m [id2label[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m pred_ids]\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m classification report:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 83\u001b[0m \u001b[43mprint_results\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdev_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpred_labels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m trainer, eval_results, pred_labels\n",
      "Cell \u001b[0;32mIn[3], line 7\u001b[0m, in \u001b[0;36mprint_results\u001b[0;34m(gold_labels, predicted_labels)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mprint_results\u001b[39m(gold_labels, predicted_labels):\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;66;03m# overall\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m     p, r, f, _ \u001b[38;5;241m=\u001b[39m \u001b[43mprecision_recall_fscore_support\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgold_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpredicted_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maverage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmacro\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mzero_division\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m     acc \u001b[38;5;241m=\u001b[39m accuracy_score(gold_labels, predicted_labels)\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=== Overall (Macro Avg) ===\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp_env/lib/python3.10/site-packages/sklearn/utils/_param_validation.py:218\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    213\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m    214\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    215\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    216\u001b[0m         )\n\u001b[1;32m    217\u001b[0m     ):\n\u001b[0;32m--> 218\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    221\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    222\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    223\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    224\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[1;32m    225\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    226\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    227\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[1;32m    228\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp_env/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1996\u001b[0m, in \u001b[0;36mprecision_recall_fscore_support\u001b[0;34m(y_true, y_pred, beta, labels, pos_label, average, warn_for, sample_weight, zero_division)\u001b[0m\n\u001b[1;32m   1827\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Compute precision, recall, F-measure and support for each class.\u001b[39;00m\n\u001b[1;32m   1828\u001b[0m \n\u001b[1;32m   1829\u001b[0m \u001b[38;5;124;03mThe precision is the ratio ``tp / (tp + fp)`` where ``tp`` is the number of\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1993\u001b[0m \u001b[38;5;124;03m array([2, 2, 2]))\u001b[39;00m\n\u001b[1;32m   1994\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1995\u001b[0m _check_zero_division(zero_division)\n\u001b[0;32m-> 1996\u001b[0m labels \u001b[38;5;241m=\u001b[39m \u001b[43m_check_set_wise_labels\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maverage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpos_label\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1998\u001b[0m \u001b[38;5;66;03m# Calculate tp_sum, pred_sum, true_sum ###\u001b[39;00m\n\u001b[1;32m   1999\u001b[0m samplewise \u001b[38;5;241m=\u001b[39m average \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msamples\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp_env/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1765\u001b[0m, in \u001b[0;36m_check_set_wise_labels\u001b[0;34m(y_true, y_pred, average, labels, pos_label)\u001b[0m\n\u001b[1;32m   1762\u001b[0m y_type, y_true, y_pred \u001b[38;5;241m=\u001b[39m _check_targets(y_true, y_pred)\n\u001b[1;32m   1763\u001b[0m \u001b[38;5;66;03m# Convert to Python primitive type to avoid NumPy type / Python str\u001b[39;00m\n\u001b[1;32m   1764\u001b[0m \u001b[38;5;66;03m# comparison. See https://github.com/numpy/numpy/issues/6784\u001b[39;00m\n\u001b[0;32m-> 1765\u001b[0m present_labels \u001b[38;5;241m=\u001b[39m _tolist(\u001b[43munique_labels\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1766\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m average \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1767\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m y_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp_env/lib/python3.10/site-packages/sklearn/utils/multiclass.py:117\u001b[0m, in \u001b[0;36munique_labels\u001b[0;34m(*ys)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;66;03m# Check that we don't mix string type with number type\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mset\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(label, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m label \u001b[38;5;129;01min\u001b[39;00m ys_labels)) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMix of label input types (string and number)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m xp\u001b[38;5;241m.\u001b[39masarray(\u001b[38;5;28msorted\u001b[39m(ys_labels))\n",
      "\u001b[0;31mValueError\u001b[0m: Mix of label input types (string and number)"
     ]
    }
   ],
   "source": [
    "distilbert_trainer, distilbert_results, distilbert_pred_labels = train_and_eval_transformer(\n",
    "    model_name=\"distilbert/distilbert-base-uncased\",\n",
    "    train_dataset=train_ds,\n",
    "    dev_dataset=dev_ds,\n",
    "    output_dir=\"./distilbert_musicmood\",\n",
    "    num_epochs=3,\n",
    "    learning_rate=5e-5,\n",
    "    train_bs=4,\n",
    "    eval_bs=4,\n",
    "    set_pad_token_eos=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03def089-c0aa-408f-a10b-416840509054",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
