{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e0dff1b",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "f8901e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Imports\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "import gensim.downloader\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8434a0e5",
   "metadata": {},
   "source": [
    "## Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "d5e8a198-92c0-4eb3-bc62-435c35a70434",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_tokenize_pattern = re.compile(r\"(?u)\\b\\w\\w+\\b\")\n",
    "\n",
    "\n",
    "def word_tokenize(s: str):\n",
    "    return [x.lower() for x in word_tokenize_pattern.findall(s)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e16e61b3",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "eb3292a6-f7ab-4858-9732-76ba312fe93d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "EMOTIONS = [\"Angry\", \"Happy\", \"Relaxed\", \"Sad\"]\n",
    "metrics_result = []\n",
    "emotions_metric_result = []\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def print_results(gold_labels, predicted_labels, model_name = \"\", display = True):\n",
    "    # overall\n",
    "    p, r, f, _ = precision_recall_fscore_support(\n",
    "        gold_labels, predicted_labels, average=\"macro\", zero_division=0\n",
    "    )\n",
    "    acc = accuracy_score(gold_labels, predicted_labels)\n",
    "\n",
    "    result = {\"Model\": model_name, \"Precision\": p, \"Recall\": r, \"F1\": f, \"Accuracy\": acc}\n",
    "    \n",
    "\n",
    "    index_found = next((i for i, d in enumerate(metrics_result) if d.get('Model') == model_name), None)\n",
    "\n",
    "    if (index_found == None):\n",
    "        metrics_result.append(result)\n",
    "    else:\n",
    "        metrics_result[index_found] = result\n",
    "\n",
    "    df_all = pd.DataFrame(metrics_result)\n",
    "  \n",
    "    if (display == True):\n",
    "        print(\"=== Overall (Macro Avg) ===\")\n",
    "        # print(\"Precision:\", p)\n",
    "        # print(\"Recall:\", r)\n",
    "        # print(\"F1:\", f)\n",
    "        # print(\"Accuracy:\", acc)\n",
    "        # print(metrics_result)\n",
    "        print(df_all.to_string(index=False))\n",
    "        print()\n",
    "\n",
    "   \n",
    "\n",
    "\n",
    "    # Per-emotion metrics\n",
    "    p_i, r_i, f_i, _ = precision_recall_fscore_support(\n",
    "        gold_labels, predicted_labels, average=None, zero_division=0\n",
    "    )\n",
    "    if (display == True):\n",
    "        print(\"=== Per Emotion (Class) Metrics ===\")\n",
    "    for i, emotion in enumerate(EMOTIONS):\n",
    "        # print(\"  Precision:\", p_i[i])\n",
    "        # print(\"  Recall:   \", r_i[i])\n",
    "        # print(\"  F1:       \", f_i[i])\n",
    "        emotion_result = {\"Model\": model_name, \"Emotion\": emotion, \"Precision\": p_i[i], \"Recall\": r_i[i], \"F1\": f_i[i]}\n",
    "        emotion_index_found = next((i for i, d in enumerate(emotions_metric_result) if d.get('Model') == model_name and d.get(\"Emotion\" == emotion)), None)\n",
    "\n",
    "        if (emotion_index_found == None):\n",
    "            emotions_metric_result.append(emotion_result)\n",
    "        else:\n",
    "            emotions_metric_result[emotion_index_found] = emotion_result\n",
    "        df_emotions = pd.DataFrame(emotions_metric_result)\n",
    "        filtered_df = df_emotions[df_emotions['Emotion'] == emotion]\n",
    "        if (display == True):\n",
    "            print(f\"{emotion}:\")\n",
    "            print(filtered_df.to_string(index=False))\n",
    "            print()\n",
    "    \n",
    "    \n",
    "    \n",
    "   \n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = logits.argmax(axis=-1)\n",
    "\n",
    "    p, r, f, _ = precision_recall_fscore_support(\n",
    "        labels, preds, average=\"macro\", zero_division=0\n",
    "    )\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\"precision\": p, \"recall\": r, \"f1\": f, \"accuracy\": acc}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da6b4b6e",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "4edfab4b-46e2-464a-a47f-a2bb86f62c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_DIR = \"NJU_MusicMood_v1.0\"\n",
    "\n",
    "label2id = {label: i for i, label in enumerate(EMOTIONS)}\n",
    "id2label = {i: label for label, i in label2id.items()}\n",
    "\n",
    "timestamp_pattern = re.compile(r\"\\[\\d{2}:\\d{2}(?:\\.\\d{2})?\\]\")\n",
    "\n",
    "\n",
    "def clean_lyrics(text: str) -> str:\n",
    "    # Remove timestamps like [00:29]\n",
    "    text = timestamp_pattern.sub(\"\", text)\n",
    "\n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Normalize quotes\n",
    "    text = text.replace(\"’\", \"'\").replace(\"“\", '\"').replace(\"”\", '\"')\n",
    "\n",
    "    # Remove ellipses and repeated dots\n",
    "    text = re.sub(r\"\\.{2,}\", \" \", text)\n",
    "\n",
    "    # Remove long underscores\n",
    "    text = re.sub(r\"_{2,}\", \" \", text)\n",
    "\n",
    "    # Remove trailing \"end\" markers at the end of the file\n",
    "    text = re.sub(r\"\\bend[.\\s]*$\", \"\", text.strip())\n",
    "\n",
    "    # Replace newlines with space\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "\n",
    "    # Keep only letters, digits, spaces, apostrophes\n",
    "    text = re.sub(r\"[^a-z0-9' ]+\", \" \", text)\n",
    "\n",
    "    # Collapse multiple spaces\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "\n",
    "def get_lyrics(path: str) -> str:\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        raw = f.read()\n",
    "    return clean_lyrics(raw)\n",
    "\n",
    "\n",
    "def get_lyrics_and_labels(split: str):\n",
    "    texts, labels = [], []\n",
    "    for emotion in EMOTIONS:\n",
    "        folder = os.path.join(DATASET_DIR, emotion, split)\n",
    "        if not os.path.isdir(folder):\n",
    "            continue\n",
    "\n",
    "        for fname in os.listdir(folder):\n",
    "            if not fname.endswith(\".txt\"):\n",
    "                continue\n",
    "            if fname.lower() == \"info.txt\":\n",
    "                continue\n",
    "\n",
    "            path = os.path.join(folder, fname)\n",
    "            txt = get_lyrics(path)\n",
    "            if txt.strip():\n",
    "                texts.append(txt)\n",
    "                labels.append(emotion)\n",
    "    return texts, labels\n",
    "\n",
    "\n",
    "# Load data \n",
    "train_texts, train_labels = get_lyrics_and_labels(\"Train\")\n",
    "dev_texts, dev_labels = get_lyrics_and_labels(\"Test\")\n",
    "\n",
    "assert len(train_texts) == len(train_labels)\n",
    "assert len(dev_texts) == len(dev_labels)\n",
    "\n",
    "# Datasets\n",
    "train_ds = Dataset.from_dict(\n",
    "    {\"text\": train_texts, \"label\": [label2id[l] for l in train_labels]}\n",
    ")\n",
    "dev_ds = Dataset.from_dict(\n",
    "    {\"text\": dev_texts, \"label\": [label2id[l] for l in dev_labels]}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f93414e",
   "metadata": {},
   "source": [
    "## Baseline: Bag of Words & Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "f5a40396-55f3-43dc-af8e-fd45ad82c685",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Baseline: Bag of Words & Logistic Regression ===\n",
      "=== Overall (Macro Avg) ===\n",
      "   Model  Precision   Recall       F1  Accuracy\n",
      "BoW & LR   0.380262 0.372645 0.373144  0.363395\n",
      "\n",
      "=== Per Emotion (Class) Metrics ===\n",
      "Angry:\n",
      "   Model Emotion  Precision   Recall       F1\n",
      "BoW & LR   Angry   0.492958 0.492958 0.492958\n",
      "\n",
      "Happy:\n",
      "   Model Emotion  Precision   Recall       F1\n",
      "BoW & LR   Happy    0.45679 0.349057 0.395722\n",
      "\n",
      "Relaxed:\n",
      "   Model Emotion  Precision  Recall       F1\n",
      "BoW & LR Relaxed   0.305344 0.39604 0.344828\n",
      "\n",
      "Sad:\n",
      "   Model Emotion  Precision   Recall       F1\n",
      "BoW & LR     Sad   0.265957 0.252525 0.259067\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=== Baseline: Bag of Words & Logistic Regression ===\")\n",
    "\n",
    "count_vectorizer = CountVectorizer(analyzer=word_tokenize)\n",
    "train_counts = count_vectorizer.fit_transform(train_texts)\n",
    "dev_counts = count_vectorizer.transform(dev_texts)\n",
    "\n",
    "lr_bow = LogisticRegression(max_iter=500, random_state=0)\n",
    "lr_bow.fit(train_counts, train_labels)\n",
    "\n",
    "lr_bow_dev_predictions = lr_bow.predict(dev_counts)\n",
    "print_results(dev_labels, lr_bow_dev_predictions, \"BoW & LR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80bab194",
   "metadata": {},
   "source": [
    "## Baseline 2: Word2Vec & Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "e417a9d1-ef6c-4527-a6dc-49d0e591d404",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Word2Vec & Logistic Regression ===\n",
      "=== Overall (Macro Avg) ===\n",
      "        Model  Precision   Recall       F1  Accuracy\n",
      "     BoW & LR   0.380262 0.372645 0.373144  0.363395\n",
      "Word2Vec & LR   0.461446 0.478161 0.455341  0.453581\n",
      "\n",
      "=== Per Emotion (Class) Metrics ===\n",
      "Angry:\n",
      "        Model Emotion  Precision   Recall       F1\n",
      "     BoW & LR   Angry   0.492958 0.492958 0.492958\n",
      "Word2Vec & LR   Angry   0.612903 0.802817 0.695122\n",
      "\n",
      "Happy:\n",
      "        Model Emotion  Precision   Recall       F1\n",
      "     BoW & LR   Happy   0.456790 0.349057 0.395722\n",
      "Word2Vec & LR   Happy   0.480392 0.462264 0.471154\n",
      "\n",
      "Relaxed:\n",
      "        Model Emotion  Precision   Recall       F1\n",
      "     BoW & LR Relaxed   0.305344 0.396040 0.344828\n",
      "Word2Vec & LR Relaxed   0.335821 0.445545 0.382979\n",
      "\n",
      "Sad:\n",
      "        Model Emotion  Precision   Recall       F1\n",
      "     BoW & LR     Sad   0.265957 0.252525 0.259067\n",
      "Word2Vec & LR     Sad   0.416667 0.202020 0.272109\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=== Word2Vec & Logistic Regression ===\")\n",
    "\n",
    "w2v_model = gensim.downloader.load(\"word2vec-google-news-300\")\n",
    "VECTOR_SIZE = w2v_model.vector_size\n",
    "\n",
    "\n",
    "def vec_for_doc(tokenized_doc):\n",
    "    vectors = [w2v_model[word] for word in tokenized_doc if word in w2v_model.key_to_index]\n",
    "    if not vectors:\n",
    "        return np.zeros(VECTOR_SIZE, dtype=\"float32\")\n",
    "    return np.mean(vectors, axis=0)\n",
    "\n",
    "\n",
    "train_vecs = [vec_for_doc(word_tokenize(x)) for x in train_texts]\n",
    "dev_vecs = [vec_for_doc(word_tokenize(x)) for x in dev_texts]\n",
    "\n",
    "lr_w2v = LogisticRegression(max_iter=500, random_state=0)\n",
    "lr_w2v.fit(train_vecs, train_labels)\n",
    "\n",
    "w2v_dev_predictions = lr_w2v.predict(dev_vecs)\n",
    "print_results(dev_labels, w2v_dev_predictions, \"Word2Vec & LR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e21eee90",
   "metadata": {},
   "source": [
    "## Split lyrics into start middle end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "d39a0021",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_segment(text, segment=\"start\", portion=0.33):\n",
    "    \"\"\"\n",
    "    Extract a portion of the lyrics.\n",
    "    portion=0.3 means 30% of tokens.\n",
    "    segment can be \"start\", \"middle\", or \"end\".\n",
    "    \"\"\"\n",
    "    tokens = text.split()\n",
    "    n = len(tokens)\n",
    "    if n == 0:\n",
    "        return \"\"\n",
    "\n",
    "    cut = int(n * portion)  # number of tokens for start/end\n",
    "\n",
    "    if segment == \"start\":\n",
    "        return \" \".join(tokens[:cut])\n",
    "\n",
    "    elif segment == \"middle\":\n",
    "        start = int(n * 0.33)\n",
    "        end = int(n * 0.66)\n",
    "        return \" \".join(tokens[start:end])\n",
    "\n",
    "    elif segment == \"end\":\n",
    "        laststart = int(n * 0.66)\n",
    "        # lastend= int(n * (portion + 0.01))\n",
    "        # return \" \".join(tokens[-lastcut:])\n",
    "        return \" \".join(tokens[laststart:n])\n",
    "\n",
    "    else:\n",
    "        return text\n",
    "\n",
    "\n",
    "# Build dev\n",
    "dev_start_texts = [get_segment(t, \"start\") for t in dev_texts]\n",
    "dev_middle_texts = [get_segment(t, \"middle\") for t in dev_texts]\n",
    "dev_end_texts = [get_segment(t, \"end\") for t in dev_texts]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c423049",
   "metadata": {},
   "source": [
    "## Evaluate BoW model Segmented Lyrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "01c265a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== BoW Logistic Regression: Position-based Evaluation (NO retraining) ===\n",
      "=== Overall (Macro Avg) ===\n",
      "               Model  Precision   Recall       F1  Accuracy\n",
      "            BoW & LR   0.380262 0.372645 0.373144  0.363395\n",
      "       Word2Vec & LR   0.461446 0.478161 0.455341  0.453581\n",
      " BoW Segmented_START   0.414294 0.320851 0.275773  0.323607\n",
      "BoW Segmented_MIDDLE   0.405777 0.314621 0.274071  0.318302\n",
      "   BoW Segmented_END   0.341373 0.301432 0.271661  0.307692\n",
      "\n",
      "=== Per Emotion (Class) Metrics ===\n",
      "Angry:\n",
      "               Model Emotion  Precision   Recall       F1\n",
      "            BoW & LR   Angry   0.492958 0.492958 0.492958\n",
      "       Word2Vec & LR   Angry   0.612903 0.802817 0.695122\n",
      " BoW Segmented_START   Angry   0.575758 0.267606 0.365385\n",
      "BoW Segmented_MIDDLE   Angry   0.562500 0.253521 0.349515\n",
      "   BoW Segmented_END   Angry   0.441176 0.211268 0.285714\n",
      "\n",
      "Happy:\n",
      "               Model Emotion  Precision   Recall       F1\n",
      "            BoW & LR   Happy   0.456790 0.349057 0.395722\n",
      "       Word2Vec & LR   Happy   0.480392 0.462264 0.471154\n",
      " BoW Segmented_START   Happy   0.600000 0.113208 0.190476\n",
      "BoW Segmented_MIDDLE   Happy   0.538462 0.132075 0.212121\n",
      "   BoW Segmented_END   Happy   0.414634 0.160377 0.231293\n",
      "\n",
      "Relaxed:\n",
      "               Model Emotion  Precision   Recall       F1\n",
      "            BoW & LR Relaxed   0.305344 0.396040 0.344828\n",
      "       Word2Vec & LR Relaxed   0.335821 0.445545 0.382979\n",
      " BoW Segmented_START Relaxed   0.295374 0.821782 0.434555\n",
      "BoW Segmented_MIDDLE Relaxed   0.279720 0.792079 0.413437\n",
      "   BoW Segmented_END Relaxed   0.291498 0.712871 0.413793\n",
      "\n",
      "Sad:\n",
      "               Model Emotion  Precision   Recall       F1\n",
      "            BoW & LR     Sad   0.265957 0.252525 0.259067\n",
      "       Word2Vec & LR     Sad   0.416667 0.202020 0.272109\n",
      " BoW Segmented_START     Sad   0.186047 0.080808 0.112676\n",
      "BoW Segmented_MIDDLE     Sad   0.242424 0.080808 0.121212\n",
      "   BoW Segmented_END     Sad   0.218182 0.121212 0.155844\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=== BoW Logistic Regression: Position-based Evaluation (NO retraining) ===\")\n",
    "\n",
    "# START\n",
    "dev_start_counts = count_vectorizer.transform(dev_start_texts,)\n",
    "bow_start_preds = lr_bow.predict(dev_start_counts)\n",
    "# print(\"\\n--- BoW on START segment only ---\")\n",
    "print_results(dev_labels, bow_start_preds, \"BoW Segmented_START\", False)\n",
    "\n",
    "# MIDDLE\n",
    "dev_middle_counts = count_vectorizer.transform(dev_middle_texts)\n",
    "bow_middle_preds = lr_bow.predict(dev_middle_counts)\n",
    "# print(\"\\n--- BoW on MIDDLE segment only ---\")\n",
    "print_results(dev_labels, bow_middle_preds, \"BoW Segmented_MIDDLE\", False)\n",
    "\n",
    "# END\n",
    "dev_end_counts = count_vectorizer.transform(dev_end_texts)\n",
    "bow_end_preds = lr_bow.predict(dev_end_counts)\n",
    "# print(\"\\n--- BoW on END segment only ---\")\n",
    "print_results(dev_labels, bow_end_preds, \"BoW Segmented_END\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f835d866",
   "metadata": {},
   "source": [
    "## Evaluate Word2Vec on Segmented Lyrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "0efcd5b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Word2Vec Logistic Regression: Position-based Evaluation (NO retraining) ===\n",
      "=== Overall (Macro Avg) ===\n",
      "                    Model  Precision   Recall       F1  Accuracy\n",
      "                 BoW & LR   0.380262 0.372645 0.373144  0.363395\n",
      "            Word2Vec & LR   0.461446 0.478161 0.455341  0.453581\n",
      "      BoW Segmented_START   0.414294 0.320851 0.275773  0.323607\n",
      "     BoW Segmented_MIDDLE   0.405777 0.314621 0.274071  0.318302\n",
      "        BoW Segmented_END   0.341373 0.301432 0.271661  0.307692\n",
      " Word2Vec Segmented_START   0.409452 0.430841 0.409294  0.411141\n",
      "Word2Vec Segmented_MIDDLE   0.427968 0.456468 0.428019  0.432361\n",
      "   Word2Vec Segmented_END   0.388004 0.411518 0.388873  0.389920\n",
      "\n",
      "=== Per Emotion (Class) Metrics ===\n",
      "Angry:\n",
      "                    Model Emotion  Precision   Recall       F1\n",
      "                 BoW & LR   Angry   0.492958 0.492958 0.492958\n",
      "            Word2Vec & LR   Angry   0.612903 0.802817 0.695122\n",
      "      BoW Segmented_START   Angry   0.575758 0.267606 0.365385\n",
      "     BoW Segmented_MIDDLE   Angry   0.562500 0.253521 0.349515\n",
      "        BoW Segmented_END   Angry   0.441176 0.211268 0.285714\n",
      " Word2Vec Segmented_START   Angry   0.505155 0.690141 0.583333\n",
      "Word2Vec Segmented_MIDDLE   Angry   0.544554 0.774648 0.639535\n",
      "   Word2Vec Segmented_END   Angry   0.520833 0.704225 0.598802\n",
      "\n",
      "Happy:\n",
      "                    Model Emotion  Precision   Recall       F1\n",
      "                 BoW & LR   Happy   0.456790 0.349057 0.395722\n",
      "            Word2Vec & LR   Happy   0.480392 0.462264 0.471154\n",
      "      BoW Segmented_START   Happy   0.600000 0.113208 0.190476\n",
      "     BoW Segmented_MIDDLE   Happy   0.538462 0.132075 0.212121\n",
      "        BoW Segmented_END   Happy   0.414634 0.160377 0.231293\n",
      " Word2Vec Segmented_START   Happy   0.434343 0.405660 0.419512\n",
      "Word2Vec Segmented_MIDDLE   Happy   0.474227 0.433962 0.453202\n",
      "   Word2Vec Segmented_END   Happy   0.401709 0.443396 0.421525\n",
      "\n",
      "Relaxed:\n",
      "                    Model Emotion  Precision   Recall       F1\n",
      "                 BoW & LR Relaxed   0.305344 0.396040 0.344828\n",
      "            Word2Vec & LR Relaxed   0.335821 0.445545 0.382979\n",
      "      BoW Segmented_START Relaxed   0.295374 0.821782 0.434555\n",
      "     BoW Segmented_MIDDLE Relaxed   0.279720 0.792079 0.413437\n",
      "        BoW Segmented_END Relaxed   0.291498 0.712871 0.413793\n",
      " Word2Vec Segmented_START Relaxed   0.346457 0.435644 0.385965\n",
      "Word2Vec Segmented_MIDDLE Relaxed   0.346154 0.445545 0.389610\n",
      "   Word2Vec Segmented_END Relaxed   0.289474 0.326733 0.306977\n",
      "\n",
      "Sad:\n",
      "                    Model Emotion  Precision   Recall       F1\n",
      "                 BoW & LR     Sad   0.265957 0.252525 0.259067\n",
      "            Word2Vec & LR     Sad   0.416667 0.202020 0.272109\n",
      "      BoW Segmented_START     Sad   0.186047 0.080808 0.112676\n",
      "     BoW Segmented_MIDDLE     Sad   0.242424 0.080808 0.121212\n",
      "        BoW Segmented_END     Sad   0.218182 0.121212 0.155844\n",
      " Word2Vec Segmented_START     Sad   0.351852 0.191919 0.248366\n",
      "Word2Vec Segmented_MIDDLE     Sad   0.346939 0.171717 0.229730\n",
      "   Word2Vec Segmented_END     Sad   0.340000 0.171717 0.228188\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=== Word2Vec Logistic Regression: Position-based Evaluation (NO retraining) ===\")\n",
    "\n",
    "# START\n",
    "dev_start_vecs = [vec_for_doc(word_tokenize(x)) for x in dev_start_texts]\n",
    "w2v_start_preds = lr_w2v.predict(dev_start_vecs)\n",
    "# print(\"\\n--- W2V on START segment only ---\")\n",
    "print_results(dev_labels, w2v_start_preds, \"Word2Vec Segmented_START\", False)\n",
    "\n",
    "# MIDDLE\n",
    "dev_middle_vecs = [vec_for_doc(word_tokenize(x)) for x in dev_middle_texts]\n",
    "w2v_middle_preds = lr_w2v.predict(dev_middle_vecs)\n",
    "# print(\"\\n--- W2V on MIDDLE segment only ---\")\n",
    "print_results(dev_labels, w2v_middle_preds, \"Word2Vec Segmented_MIDDLE\", False)\n",
    "\n",
    "# END\n",
    "dev_end_vecs = [vec_for_doc(word_tokenize(x)) for x in dev_end_texts]\n",
    "w2v_end_preds = lr_w2v.predict(dev_end_vecs)\n",
    "# print(\"\\n--- W2V on END segment only ---\")\n",
    "print_results(dev_labels, w2v_end_preds, \"Word2Vec Segmented_END\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "122a5a0d",
   "metadata": {},
   "source": [
    "## Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "8275c4c9-0c1f-4969-a64a-e055131ce792",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_dataset(dataset, tokenizer, max_length: int = 256):\n",
    "    def _tok(batch):\n",
    "        return tokenizer(\n",
    "            batch[\"text\"],\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=max_length,\n",
    "        )\n",
    "\n",
    "    tokenized = dataset.map(_tok, batched=True)\n",
    "    tokenized = tokenized.remove_columns([\"text\"])\n",
    "    tokenized.set_format(type=\"torch\")\n",
    "    return tokenized\n",
    "\n",
    "\n",
    "def train_and_eval_transformer(\n",
    "    model_name: str,\n",
    "    train_dataset: Dataset,\n",
    "    dev_dataset: Dataset,\n",
    "    output_dir: str,\n",
    "    num_epochs: int,\n",
    "    learning_rate: float,\n",
    "    train_bs: int,\n",
    "    eval_bs: int,\n",
    "    model_directory: str,\n",
    "    set_pad_token_eos: bool = False,\n",
    "):\n",
    "    print(f\"=== Fine-tuning {model_name} ===\")\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    if set_pad_token_eos:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    tokenized_train = tokenize_dataset(train_dataset, tokenizer)\n",
    "    tokenized_dev = tokenize_dataset(dev_dataset, tokenizer)\n",
    "\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_name,\n",
    "        num_labels=len(EMOTIONS),\n",
    "        label2id=label2id,\n",
    "        id2label=id2label,\n",
    "        ignore_mismatched_sizes=True,\n",
    "    )\n",
    "\n",
    "    if set_pad_token_eos:\n",
    "        model.config.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        num_train_epochs=num_epochs,\n",
    "        learning_rate=learning_rate,\n",
    "        per_device_train_batch_size=train_bs,\n",
    "        per_device_eval_batch_size=eval_bs,\n",
    "        weight_decay=0.01,\n",
    "        eval_strategy=\"epoch\",\n",
    "        logging_steps=16,\n",
    "        log_level=\"error\",\n",
    "        report_to=\"none\",\n",
    "        save_strategy=\"epoch\",\n",
    "        dataloader_pin_memory=False,\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_train,\n",
    "        eval_dataset=tokenized_dev,\n",
    "        compute_metrics=compute_metrics,\n",
    "        tokenizer=tokenizer,\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "    eval_results = trainer.evaluate()\n",
    "    print(f\"{model_name} dev results:\", eval_results)\n",
    "\n",
    "    pred_output = trainer.predict(tokenized_dev)\n",
    "    logits = pred_output.predictions\n",
    "    pred_ids = np.argmax(logits, axis=-1)\n",
    "    pred_labels = [id2label[i] for i in pred_ids]\n",
    "\n",
    "    print(f\"{model_name} classification report:\")\n",
    "    print_results(dev_labels, pred_labels, model_name)\n",
    "\n",
    "    trainer.save_model(model_directory)\n",
    "    tokenizer.save_pretrained(model_directory)\n",
    "\n",
    "    return trainer, eval_results, pred_labels\n",
    "\n",
    "def eval_transformer_on_segments(model_name: str, trainer: Trainer):\n",
    "    \"\"\"\n",
    "    Evaluate a fine-tuned transformer (trainer) on START/MIDDLE/END\n",
    "    segments of the dev set, without retraining.\n",
    "    \"\"\"\n",
    "    tokenizer = trainer.tokenizer\n",
    "\n",
    "    for seg in [\"start\", \"middle\", \"end\"]:\n",
    "        # Build segmented dev texts\n",
    "        seg_texts = [get_segment(t, seg) for t in dev_texts]\n",
    "\n",
    "        # Build a segmented dev Dataset with same labels\n",
    "        seg_dev_ds = Dataset.from_dict(\n",
    "            {\n",
    "                \"text\": seg_texts,\n",
    "                \"label\": [label2id[l] for l in dev_labels],\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # Tokenize segmented dataset\n",
    "        tokenized_seg_dev = tokenize_dataset(seg_dev_ds, tokenizer)\n",
    "\n",
    "        # Predict\n",
    "        pred_output = trainer.predict(tokenized_seg_dev)\n",
    "        logits = pred_output.predictions\n",
    "        pred_ids = np.argmax(logits, axis=-1)\n",
    "        pred_labels = [id2label[i] for i in pred_ids]\n",
    "\n",
    "        print(f\"\\n=== {model_name} on {seg.upper()} segment only ===\")\n",
    "        # print_results(dev_labels, pred_labels, model_name, False)\n",
    "        if (seg == \"end\"):\n",
    "            print_results(dev_labels, pred_labels, f\"{model_name}_{seg}\")\n",
    "        else:\n",
    "            print_results(dev_labels, pred_labels, f\"{model_name}_{seg}\", False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "221d4e43",
   "metadata": {},
   "source": [
    "## Distilbert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "fe4e592e-e7ad-4d23-b757-580470945074",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained Model existing in directory 'distilbert_model'\n",
      "The saved model 'distilbert_model' deleted successfully.\n",
      "=== Fine-tuning distilbert/distilbert-base-uncased ===\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8882dad1149949e9b64eb82d6337db7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/400 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb9d154b2a1b4db8baaffe7d68c66e46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/377 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_45776\\835601709.py:64: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5069, 'grad_norm': 5.180815696716309, 'learning_rate': 4.75e-05, 'epoch': 0.16}\n",
      "{'loss': 0.83, 'grad_norm': 7.2908711433410645, 'learning_rate': 4.483333333333333e-05, 'epoch': 0.32}\n",
      "{'loss': 0.7159, 'grad_norm': 4.195237159729004, 'learning_rate': 4.216666666666667e-05, 'epoch': 0.48}\n",
      "{'loss': 0.5563, 'grad_norm': 21.9300479888916, 'learning_rate': 3.9500000000000005e-05, 'epoch': 0.64}\n",
      "{'loss': 0.5397, 'grad_norm': 29.086624145507812, 'learning_rate': 3.683333333333334e-05, 'epoch': 0.8}\n",
      "{'loss': 0.7083, 'grad_norm': 9.722677230834961, 'learning_rate': 3.4166666666666666e-05, 'epoch': 0.96}\n",
      "{'eval_loss': 1.7001217603683472, 'eval_precision': 0.4511138463131101, 'eval_recall': 0.4570951547719215, 'eval_f1': 0.42132056848901406, 'eval_accuracy': 0.4297082228116711, 'eval_runtime': 62.6141, 'eval_samples_per_second': 6.021, 'eval_steps_per_second': 1.517, 'epoch': 1.0}\n",
      "{'loss': 0.7769, 'grad_norm': 18.317607879638672, 'learning_rate': 3.15e-05, 'epoch': 1.12}\n",
      "{'loss': 0.4088, 'grad_norm': 2.104121446609497, 'learning_rate': 2.8833333333333334e-05, 'epoch': 1.28}\n",
      "{'loss': 0.4066, 'grad_norm': 17.146291732788086, 'learning_rate': 2.6166666666666668e-05, 'epoch': 1.44}\n",
      "{'loss': 0.4152, 'grad_norm': 14.018135070800781, 'learning_rate': 2.35e-05, 'epoch': 1.6}\n",
      "{'loss': 0.3975, 'grad_norm': 13.652917861938477, 'learning_rate': 2.0833333333333336e-05, 'epoch': 1.76}\n",
      "{'loss': 0.338, 'grad_norm': 15.093550682067871, 'learning_rate': 1.8166666666666667e-05, 'epoch': 1.92}\n",
      "{'eval_loss': 2.1932997703552246, 'eval_precision': 0.4765290607183339, 'eval_recall': 0.5014673803282533, 'eval_f1': 0.47735517758024576, 'eval_accuracy': 0.47480106100795755, 'eval_runtime': 66.6559, 'eval_samples_per_second': 5.656, 'eval_steps_per_second': 1.425, 'epoch': 2.0}\n",
      "{'loss': 0.2644, 'grad_norm': 0.7288429141044617, 'learning_rate': 1.55e-05, 'epoch': 2.08}\n",
      "{'loss': 0.2092, 'grad_norm': 0.4850783348083496, 'learning_rate': 1.2833333333333333e-05, 'epoch': 2.24}\n",
      "{'loss': 0.1886, 'grad_norm': 4.8744401931762695, 'learning_rate': 1.0166666666666667e-05, 'epoch': 2.4}\n",
      "{'loss': 0.2609, 'grad_norm': 7.364206314086914, 'learning_rate': 7.5e-06, 'epoch': 2.56}\n",
      "{'loss': 0.2617, 'grad_norm': 1.426767110824585, 'learning_rate': 4.833333333333333e-06, 'epoch': 2.7199999999999998}\n",
      "{'loss': 0.3274, 'grad_norm': 0.21895846724510193, 'learning_rate': 2.166666666666667e-06, 'epoch': 2.88}\n",
      "{'eval_loss': 2.320039749145508, 'eval_precision': 0.48409060829508926, 'eval_recall': 0.4898795652884449, 'eval_f1': 0.48012781934602844, 'eval_accuracy': 0.46684350132625996, 'eval_runtime': 77.6068, 'eval_samples_per_second': 4.858, 'eval_steps_per_second': 1.224, 'epoch': 3.0}\n",
      "{'train_runtime': 1134.9115, 'train_samples_per_second': 1.057, 'train_steps_per_second': 0.264, 'train_loss': 0.44390164136886595, 'epoch': 3.0}\n",
      "{'eval_loss': 2.320039749145508, 'eval_precision': 0.48409060829508926, 'eval_recall': 0.4898795652884449, 'eval_f1': 0.48012781934602844, 'eval_accuracy': 0.46684350132625996, 'eval_runtime': 76.7504, 'eval_samples_per_second': 4.912, 'eval_steps_per_second': 1.238, 'epoch': 3.0}\n",
      "distilbert/distilbert-base-uncased dev results: {'eval_loss': 2.320039749145508, 'eval_precision': 0.48409060829508926, 'eval_recall': 0.4898795652884449, 'eval_f1': 0.48012781934602844, 'eval_accuracy': 0.46684350132625996, 'eval_runtime': 76.7504, 'eval_samples_per_second': 4.912, 'eval_steps_per_second': 1.238, 'epoch': 3.0}\n",
      "distilbert/distilbert-base-uncased classification report:\n",
      "=== Overall (Macro Avg) ===\n",
      "                             Model  Precision   Recall       F1  Accuracy\n",
      "                          BoW & LR   0.380262 0.372645 0.373144  0.363395\n",
      "                     Word2Vec & LR   0.461446 0.478161 0.455341  0.453581\n",
      "               BoW Segmented_START   0.414294 0.320851 0.275773  0.323607\n",
      "              BoW Segmented_MIDDLE   0.405777 0.314621 0.274071  0.318302\n",
      "                 BoW Segmented_END   0.341373 0.301432 0.271661  0.307692\n",
      "          Word2Vec Segmented_START   0.409452 0.430841 0.409294  0.411141\n",
      "         Word2Vec Segmented_MIDDLE   0.427968 0.456468 0.428019  0.432361\n",
      "            Word2Vec Segmented_END   0.388004 0.411518 0.388873  0.389920\n",
      "distilbert/distilbert-base-uncased   0.484091 0.489880 0.480128  0.466844\n",
      "\n",
      "=== Per Emotion (Class) Metrics ===\n",
      "Angry:\n",
      "                             Model Emotion  Precision   Recall       F1\n",
      "                          BoW & LR   Angry   0.492958 0.492958 0.492958\n",
      "                     Word2Vec & LR   Angry   0.612903 0.802817 0.695122\n",
      "               BoW Segmented_START   Angry   0.575758 0.267606 0.365385\n",
      "              BoW Segmented_MIDDLE   Angry   0.562500 0.253521 0.349515\n",
      "                 BoW Segmented_END   Angry   0.441176 0.211268 0.285714\n",
      "          Word2Vec Segmented_START   Angry   0.505155 0.690141 0.583333\n",
      "         Word2Vec Segmented_MIDDLE   Angry   0.544554 0.774648 0.639535\n",
      "            Word2Vec Segmented_END   Angry   0.520833 0.704225 0.598802\n",
      "distilbert/distilbert-base-uncased   Angry   0.670732 0.774648 0.718954\n",
      "\n",
      "Happy:\n",
      "                             Model Emotion  Precision   Recall       F1\n",
      "                          BoW & LR   Happy   0.456790 0.349057 0.395722\n",
      "                     Word2Vec & LR   Happy   0.480392 0.462264 0.471154\n",
      "               BoW Segmented_START   Happy   0.600000 0.113208 0.190476\n",
      "              BoW Segmented_MIDDLE   Happy   0.538462 0.132075 0.212121\n",
      "                 BoW Segmented_END   Happy   0.414634 0.160377 0.231293\n",
      "          Word2Vec Segmented_START   Happy   0.434343 0.405660 0.419512\n",
      "         Word2Vec Segmented_MIDDLE   Happy   0.474227 0.433962 0.453202\n",
      "            Word2Vec Segmented_END   Happy   0.401709 0.443396 0.421525\n",
      "distilbert/distilbert-base-uncased   Happy   0.470000 0.443396 0.456311\n",
      "\n",
      "Relaxed:\n",
      "                             Model Emotion  Precision   Recall       F1\n",
      "                          BoW & LR Relaxed   0.305344 0.396040 0.344828\n",
      "                     Word2Vec & LR Relaxed   0.335821 0.445545 0.382979\n",
      "               BoW Segmented_START Relaxed   0.295374 0.821782 0.434555\n",
      "              BoW Segmented_MIDDLE Relaxed   0.279720 0.792079 0.413437\n",
      "                 BoW Segmented_END Relaxed   0.291498 0.712871 0.413793\n",
      "          Word2Vec Segmented_START Relaxed   0.346457 0.435644 0.385965\n",
      "         Word2Vec Segmented_MIDDLE Relaxed   0.346154 0.445545 0.389610\n",
      "            Word2Vec Segmented_END Relaxed   0.289474 0.326733 0.306977\n",
      "distilbert/distilbert-base-uncased Relaxed   0.454545 0.297030 0.359281\n",
      "\n",
      "Sad:\n",
      "                             Model Emotion  Precision   Recall       F1\n",
      "                          BoW & LR     Sad   0.265957 0.252525 0.259067\n",
      "                     Word2Vec & LR     Sad   0.416667 0.202020 0.272109\n",
      "               BoW Segmented_START     Sad   0.186047 0.080808 0.112676\n",
      "              BoW Segmented_MIDDLE     Sad   0.242424 0.080808 0.121212\n",
      "                 BoW Segmented_END     Sad   0.218182 0.121212 0.155844\n",
      "          Word2Vec Segmented_START     Sad   0.351852 0.191919 0.248366\n",
      "         Word2Vec Segmented_MIDDLE     Sad   0.346939 0.171717 0.229730\n",
      "            Word2Vec Segmented_END     Sad   0.340000 0.171717 0.228188\n",
      "distilbert/distilbert-base-uncased     Sad   0.341085 0.444444 0.385965\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "model_name = \"distilbert/distilbert-base-uncased\"\n",
    "distilbert_model_directory = \"distilbert_model\"\n",
    "if os.path.isdir(model_name):\n",
    "    print(f\"Trained Model existing in directory '{distilbert_model_directory}'\")\n",
    "    try:\n",
    "        # os.rmdir(distilbert_model_directory)\n",
    "        shutil.rmtree(distilbert_model_directory)\n",
    "        print(f\"The saved model '{distilbert_model_directory}' deleted successfully.\")\n",
    "    except OSError as e:\n",
    "        print(f\"Error deleting folder '{distilbert_model_directory}': {e}\")\n",
    "else:\n",
    "    print(f\"The directory '{distilbert_model_directory}' does not exist.\")\n",
    "\n",
    "distilbert_trainer, distilbert_results, distilbert_pred_labels = train_and_eval_transformer(\n",
    "    model_name=model_name,\n",
    "    train_dataset=train_ds,\n",
    "    dev_dataset=dev_ds,\n",
    "    output_dir=\"./distilbert_musicmood\",\n",
    "    num_epochs=3,\n",
    "    learning_rate=5e-5,\n",
    "    train_bs=4,\n",
    "    eval_bs=4,\n",
    "    model_directory=distilbert_model_directory,\n",
    "    set_pad_token_eos=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "087b68de",
   "metadata": {},
   "source": [
    "## Segment Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "d96d64d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6583862593404ef5854368040a6e403d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/377 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== DistilBERT on START segment only ===\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1547d56e9f754722a1d7d3e81e6f1da5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/377 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== DistilBERT on MIDDLE segment only ===\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d919f08d5dc442688ae3ecbf3d4f827",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/377 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== DistilBERT on END segment only ===\n",
      "=== Overall (Macro Avg) ===\n",
      "                             Model  Precision   Recall       F1  Accuracy\n",
      "                          BoW & LR   0.380262 0.372645 0.373144  0.363395\n",
      "                     Word2Vec & LR   0.461446 0.478161 0.455341  0.453581\n",
      "               BoW Segmented_START   0.414294 0.320851 0.275773  0.323607\n",
      "              BoW Segmented_MIDDLE   0.405777 0.314621 0.274071  0.318302\n",
      "                 BoW Segmented_END   0.341373 0.301432 0.271661  0.307692\n",
      "          Word2Vec Segmented_START   0.409452 0.430841 0.409294  0.411141\n",
      "         Word2Vec Segmented_MIDDLE   0.427968 0.456468 0.428019  0.432361\n",
      "            Word2Vec Segmented_END   0.388004 0.411518 0.388873  0.389920\n",
      "distilbert/distilbert-base-uncased   0.484091 0.489880 0.480128  0.466844\n",
      "                  DistilBERT_start   0.458753 0.437947 0.438667  0.424403\n",
      "                 DistilBERT_middle   0.464618 0.439572 0.443011  0.424403\n",
      "                    DistilBERT_end   0.488613 0.462395 0.471442  0.448276\n",
      "\n",
      "=== Per Emotion (Class) Metrics ===\n",
      "Angry:\n",
      "                             Model Emotion  Precision   Recall       F1\n",
      "                          BoW & LR   Angry   0.492958 0.492958 0.492958\n",
      "                     Word2Vec & LR   Angry   0.612903 0.802817 0.695122\n",
      "               BoW Segmented_START   Angry   0.575758 0.267606 0.365385\n",
      "              BoW Segmented_MIDDLE   Angry   0.562500 0.253521 0.349515\n",
      "                 BoW Segmented_END   Angry   0.441176 0.211268 0.285714\n",
      "          Word2Vec Segmented_START   Angry   0.505155 0.690141 0.583333\n",
      "         Word2Vec Segmented_MIDDLE   Angry   0.544554 0.774648 0.639535\n",
      "            Word2Vec Segmented_END   Angry   0.520833 0.704225 0.598802\n",
      "distilbert/distilbert-base-uncased   Angry   0.670732 0.774648 0.718954\n",
      "                  DistilBERT_start   Angry   0.666667 0.591549 0.626866\n",
      "                 DistilBERT_middle   Angry   0.771930 0.619718 0.687500\n",
      "                    DistilBERT_end   Angry   0.762712 0.633803 0.692308\n",
      "\n",
      "Happy:\n",
      "                             Model Emotion  Precision   Recall       F1\n",
      "                          BoW & LR   Happy   0.456790 0.349057 0.395722\n",
      "                     Word2Vec & LR   Happy   0.480392 0.462264 0.471154\n",
      "               BoW Segmented_START   Happy   0.600000 0.113208 0.190476\n",
      "              BoW Segmented_MIDDLE   Happy   0.538462 0.132075 0.212121\n",
      "                 BoW Segmented_END   Happy   0.414634 0.160377 0.231293\n",
      "          Word2Vec Segmented_START   Happy   0.434343 0.405660 0.419512\n",
      "         Word2Vec Segmented_MIDDLE   Happy   0.474227 0.433962 0.453202\n",
      "            Word2Vec Segmented_END   Happy   0.401709 0.443396 0.421525\n",
      "distilbert/distilbert-base-uncased   Happy   0.470000 0.443396 0.456311\n",
      "                  DistilBERT_start   Happy   0.484375 0.292453 0.364706\n",
      "                 DistilBERT_middle   Happy   0.432432 0.301887 0.355556\n",
      "                    DistilBERT_end   Happy   0.482353 0.386792 0.429319\n",
      "\n",
      "Relaxed:\n",
      "                             Model Emotion  Precision   Recall       F1\n",
      "                          BoW & LR Relaxed   0.305344 0.396040 0.344828\n",
      "                     Word2Vec & LR Relaxed   0.335821 0.445545 0.382979\n",
      "               BoW Segmented_START Relaxed   0.295374 0.821782 0.434555\n",
      "              BoW Segmented_MIDDLE Relaxed   0.279720 0.792079 0.413437\n",
      "                 BoW Segmented_END Relaxed   0.291498 0.712871 0.413793\n",
      "          Word2Vec Segmented_START Relaxed   0.346457 0.435644 0.385965\n",
      "         Word2Vec Segmented_MIDDLE Relaxed   0.346154 0.445545 0.389610\n",
      "            Word2Vec Segmented_END Relaxed   0.289474 0.326733 0.306977\n",
      "distilbert/distilbert-base-uncased Relaxed   0.454545 0.297030 0.359281\n",
      "                  DistilBERT_start Relaxed   0.395683 0.544554 0.458333\n",
      "                 DistilBERT_middle Relaxed   0.404110 0.584158 0.477733\n",
      "                    DistilBERT_end Relaxed   0.382114 0.465347 0.419643\n",
      "\n",
      "Sad:\n",
      "                             Model Emotion  Precision   Recall       F1\n",
      "                          BoW & LR     Sad   0.265957 0.252525 0.259067\n",
      "                     Word2Vec & LR     Sad   0.416667 0.202020 0.272109\n",
      "               BoW Segmented_START     Sad   0.186047 0.080808 0.112676\n",
      "              BoW Segmented_MIDDLE     Sad   0.242424 0.080808 0.121212\n",
      "                 BoW Segmented_END     Sad   0.218182 0.121212 0.155844\n",
      "          Word2Vec Segmented_START     Sad   0.351852 0.191919 0.248366\n",
      "         Word2Vec Segmented_MIDDLE     Sad   0.346939 0.171717 0.229730\n",
      "            Word2Vec Segmented_END     Sad   0.340000 0.171717 0.228188\n",
      "distilbert/distilbert-base-uncased     Sad   0.341085 0.444444 0.385965\n",
      "                  DistilBERT_start     Sad   0.288288 0.323232 0.304762\n",
      "                 DistilBERT_middle     Sad   0.250000 0.252525 0.251256\n",
      "                    DistilBERT_end     Sad   0.327273 0.363636 0.344498\n",
      "\n"
     ]
    }
   ],
   "source": [
    "eval_transformer_on_segments(\"DistilBERT\", distilbert_trainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b83701",
   "metadata": {},
   "source": [
    "## Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "eb8ed90b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer loaded successfully.\n",
      "[{'label': 'Angry', 'score': 0.9966964721679688}]\n",
      "\n",
      "=== Prediction on START segment ===\n",
      "[{'label': 'Angry', 'score': 0.9889976382255554}]\n",
      "\n",
      "\n",
      "=== Prediction on MIDDLE segment ===\n",
      "[{'label': 'Relaxed', 'score': 0.9770669341087341}]\n",
      "\n",
      "\n",
      "=== Prediction on END segment ===\n",
      "[{'label': 'Angry', 'score': 0.9876084327697754}]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "try:\n",
    "\n",
    "    # Load the model\n",
    "    loaded_model = AutoModelForSequenceClassification.from_pretrained(distilbert_model_directory)\n",
    "\n",
    "    # Load the tokenizer\n",
    "    loaded_tokenizer = AutoTokenizer.from_pretrained(distilbert_model_directory)\n",
    "    print(\"Model and tokenizer loaded successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading model: {e}\")\n",
    "\n",
    "# Create a pipeline using the loaded model and tokenizer\n",
    "nlp = pipeline(\"text-classification\", model=loaded_model, tokenizer=loaded_tokenizer)\n",
    "\n",
    "# Example predictioni\n",
    "lyrics_to_classify = \"\"\"Every time we lie awake\n",
    "After every hit we take\n",
    "Every feeling that I get\n",
    "But I haven't missed you yet\n",
    "\n",
    "Every roommate kept awake\n",
    "By every sigh and scream we make\n",
    "All the feelings that I get\n",
    "But I still don't miss you yet\n",
    "\n",
    "Only when I stop to think about it\n",
    "\n",
    "I hate everything about you\n",
    "Why do I love you?\n",
    "I hate everything about you\n",
    "Why do I love you?\n",
    "\n",
    "Every time we lie awake\n",
    "After every hit we take\n",
    "Every feeling that I get\n",
    "But I haven't missed you yet\n",
    "\n",
    "Only when I stop to think about it\n",
    "\n",
    "I hate everything about you\n",
    "Why do I love you?\n",
    "I hate everything about you\n",
    "Why do I love you?\n",
    "\n",
    "Only when I stop to think about you\n",
    "I know\n",
    "Only when you stop to think about me\n",
    "Do you know?\n",
    "\n",
    "I hate everything about you\n",
    "Why do I love you?\n",
    "You hate everything about me\n",
    "Why do you love me?\n",
    "\n",
    "I hate\n",
    "You hate\n",
    "I hate\n",
    "You love me\n",
    "\n",
    "I hate everything about you\n",
    "Why do I love you\n",
    "\"\"\"\n",
    "\n",
    "prediction = nlp(lyrics_to_classify)\n",
    "print(prediction)\n",
    "\n",
    "# Segment Prediction\n",
    "for seg in [\"start\", \"middle\", \"end\"]:\n",
    "    seg_text = get_segment(lyrics_to_classify, seg)\n",
    "    print(f\"\\n=== Prediction on {seg.upper()} segment ===\")\n",
    "    # print(seg_text)\n",
    "    seg_prediction = nlp(seg_text)\n",
    "    print(seg_prediction)\n",
    "    print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec401e5",
   "metadata": {},
   "source": [
    "## Sample Lyrics\n",
    "**Ed Sheeran - Perfect**\n",
    "\n",
    "\"\"\"I found a love for me\n",
    "Darling just dive right in\n",
    "And follow my lead\n",
    "Well I found a girl beautiful and sweet \n",
    "I never knew you were the someone waiting for me\n",
    "'Cause we were just kids when we fell in love\n",
    "\n",
    "Not knowing what it was\n",
    "I will not give you up this time\n",
    "But darling, just kiss me slow, your heart is all I own\n",
    "And in your eyes you're holding mine\n",
    "\n",
    "Baby, I'm dancing in the dark with you between my arms\n",
    "Barefoot on the grass, listening to our favorite song\n",
    "When you said you alooked  mess, I whispered underneath my breath\n",
    "But you heard it, darling, you look perfect tonight\n",
    "\n",
    "Well I found a woman, stronger than anyone I know\n",
    "She shares my dreams, I hope that someday I'll share her home\n",
    "I found a love, to carry more than just my secrets\n",
    "To carry love, to carry children of our own\n",
    "We are still kids, but we're so in love\n",
    "Fighting against all odds\n",
    "I know we'll be alright this time\n",
    "Darling, just hold my hand\n",
    "Be my girl, I'll be your man\n",
    "I see my future in your eyes\n",
    "\n",
    "Baby, I'm dancing in the dark, with you between my arms\n",
    "Barefoot on the grass, listening to our favorite song\n",
    "When I saw you in that dress, looking so beautiful\n",
    "I don't deserve this, darling, you look perfect tonight\n",
    "\n",
    "Baby, I'm dancing in the dark, with you between my arms\n",
    "Barefoot on the grass, listening to our favorite song\n",
    "I have faith in what I see\n",
    "Now I know I have met an angel in person\n",
    "And she looks perfect\n",
    "I don't deserve this\n",
    "You look perfect tonight\"\"\"\n",
    "\n",
    "\n",
    "**Pharrell Williams - Happy**\n",
    "\n",
    "\n",
    "\"\"\"It might seem crazy what I'm about to say\n",
    "Sunshine she's here, you can take a break\n",
    "I'm a hot air balloon that could go to space\n",
    "With the air, like I don't care, baby, by the way\n",
    "\n",
    "Uh\n",
    "\n",
    "(Because I'm happy)\n",
    "Clap along if you feel like a room without a roof\n",
    "(Because I'm happy)\n",
    "Clap along if you feel like happiness is the truth\n",
    "(Because I'm happy)\n",
    "Clap along if you know what happiness is to you\n",
    "(Because I'm happy)\n",
    "Clap along if you feel like that's what you wanna do\n",
    "\n",
    "Here come bad news, talking this and that (Yeah!)\n",
    "Well, give me all you got, don't hold it back (Yeah!)\n",
    "Well, I should probably warn you I'll be just fine (Yeah!)\n",
    "No offense to you\n",
    "Don't waste your time, here's why\n",
    "\n",
    "(Because I'm happy)\n",
    "Clap along if you feel like a room without a roof\n",
    "(Because I'm happy)\n",
    "Clap along if you feel like happiness is the truth\n",
    "(Because I'm happy)\n",
    "Clap along if you know what happiness is to you\n",
    "(Because I'm happy)\n",
    "Clap along if you feel like that's what you wanna do\n",
    "\n",
    "Bring me down\n",
    "Can't nothing bring me down\n",
    "My level's too high to bring me down\n",
    "Can't nothing bring me down, I said\n",
    "\n",
    "\n",
    "(Because I'm happy)\n",
    "Clap along if you feel like a room without a roof\n",
    "(Because I'm happy)\n",
    "Clap along if you feel like happiness is the truth\n",
    "(Because I'm happy)\n",
    "Clap along if you know what happiness is to you\n",
    "(Because I'm happy)\n",
    "Clap along if you feel like that's what you wanna do\n",
    "\n",
    "\n",
    "Bring me down\n",
    "Can't nothing bring me down\n",
    "My level's too high to bring me down\n",
    "Can't nothing bring me down, I said\n",
    "\n",
    "(Because I'm happy)\n",
    "Clap along if you feel like a room without a roof\n",
    "(Because I'm happy)\n",
    "Clap along if you feel like happiness is the truth\n",
    "(Because I'm happy)\n",
    "Clap along if you know what happiness is to you\n",
    "(Because I'm happy)\n",
    "Clap along if you feel like that's what you wanna do\n",
    "(Because I'm happy)\"\"\"\n",
    "\n",
    "\n",
    "**Katrina And The Waves Lyrics - I am Walking on Sunshine**\n",
    "\n",
    "\n",
    "\"\"\"I used to think maybe you loved me, now, baby, I'm sure\n",
    "And I just can't wait till the day when you knock on my door\n",
    "Now every time I go for the mailbox, gotta hold myself down\n",
    "'Cause I just can't wait till you write me you're coming around\n",
    "\n",
    "I'm walking on sunshine, whoa\n",
    "I'm walking on sunshine, whoa\n",
    "I'm walking on sunshine, whoa\n",
    "And don't it feel good\n",
    "Hey, all right now\n",
    "And don't it feel good\n",
    "Hey, yeah\n",
    "\n",
    "I used to think maybe you loved me, now I know that it's true\n",
    "And I don't want to spend my whole life just a-waiting for you\n",
    "Now, I don't want you back for the weekend, not back for a day, no, no, no\n",
    "I said, baby, I just want you back, and I want you to stay\n",
    "\n",
    "Oh, yeah, now I'm walking on sunshine, whoa\n",
    "I'm walking on sunshine, whoa\n",
    "I'm walking on sunshine, whoa\n",
    "And don't it feel good\n",
    "Hey, all right now\n",
    "And don't it feel good\n",
    "Yeah, oh, yeah, now\n",
    "And don't it feel good\n",
    "\n",
    "Walking on sunshine\n",
    "Walking on sunshine\n",
    "\n",
    "I feel alive, I feel the love, I feel the love that's really real\n",
    "I feel alive, I feel the love, I feel the love that's really real\n",
    "I'm on sunshine, baby, oh\n",
    "Oh, yeah, I'm on sunshine, baby\n",
    "\n",
    "Oh, I'm walking on sunshine, whoa\n",
    "I'm walking on sunshine, whoa\n",
    "I'm walking on sunshine, whoa\n",
    "And don't it feel good\n",
    "Hey, all right now\n",
    "And don't it feel good\n",
    "I'll say it, I'll say it, I'll say it again now\n",
    "And don't it feel good\n",
    "Hey, yeah now\n",
    "And don't it feel good\n",
    "Now don't it, don't it, don't it, don't it, don't it, don't it\n",
    "And don't it feel good\"\"\"\n",
    "\n",
    "\n",
    "**Adele - Someone Like you**\n",
    "\n",
    "\"\"\"I heard that you're settled down\n",
    "That you found a girl and you're married now.\n",
    "I heard that your dreams came true.\n",
    "Guess she gave you things I didn't give to you.\n",
    "\n",
    "Old friend, why are you so shy?\n",
    "Ain't like you to hold back or hide from the light.\n",
    "\n",
    "I hate to turn up out of the blue uninvited\n",
    "But I couldn't stay away, I couldn't fight it.\n",
    "I had hoped you'd see my face and that you'd be reminded\n",
    "That for me it isn't over.\n",
    "\n",
    "Never mind, I'll find someone like you\n",
    "I wish nothing but the best for you too\n",
    "Don't forget me, I beg\n",
    "I'll remember you said,\n",
    "\"Sometimes it lasts in love but sometimes it hurts instead,\n",
    "Sometimes it lasts in love but sometimes it hurts instead\"\n",
    "\n",
    "You know how the time flies\n",
    "Only yesterday was the time of our lives\n",
    "We were born and raised\n",
    "In a summer haze\n",
    "Bound by the surprise of our glory days\n",
    "\n",
    "I hate to turn up out of the blue uninvited\n",
    "But I couldn't stay away, I couldn't fight it.\n",
    "I'd hoped you'd see my face and that you'd be reminded\n",
    "That for me it isn't over.\n",
    "\n",
    "Never mind, I'll find someone like you\n",
    "I wish nothing but the best for you too\n",
    "Don't forget me, I beg\n",
    "I'll remember you said,\n",
    "\"Sometimes it lasts in love but sometimes it hurts instead.\"\n",
    "\n",
    "Nothing compares\n",
    "No worries or cares\n",
    "Regrets and mistakes\n",
    "They are memories made.\n",
    "Who would have known how bittersweet this would taste?\n",
    "\n",
    "Never mind, I'll find someone like you\n",
    "I wish nothing but the best for you too\n",
    "Don't forget me, I beg\n",
    "I'll remember you said,\n",
    "\"Sometimes it lasts in love but sometimes it hurts instead,\n",
    "Sometimes it lasts in love but sometimes it hurts instead.\"\"\"\n",
    "\n",
    "**Dolly Parton - Jolene** \n",
    "\n",
    "\"\"\"Jolene, Jolene, Jolene, Jolene\n",
    "I'm begging of you please don't take my man\n",
    "Jolene, Jolene, Jolene, Jolene\n",
    "Please don't take him just because you can\n",
    "\n",
    "Your beauty is beyond compare\n",
    "With flaming locks of auburn hair\n",
    "With ivory skin and eyes of emerald green\n",
    "\n",
    "Your smile is like a breath of spring\n",
    "Your voice is soft like summer rain\n",
    "And I cannot compete with you, Jolene\n",
    "\n",
    "He talks about you in his sleep\n",
    "There's nothing I can do to keep\n",
    "From crying when he calls your name, Jolene\n",
    "\n",
    "And I can easily understand\n",
    "How you could easily take my man\n",
    "But you don't know what he means to me, Jolene\n",
    "\n",
    "Jolene, Jolene, Jolene, Jolene\n",
    "I'm begging of you please don't take my man\n",
    "Jolene, Jolene, Jolene, Jolene\n",
    "Please don't take him just because you can\n",
    "\n",
    "You could have your choice of men\n",
    "But I could never love again\n",
    "He's the only one for me, Jolene\n",
    "\n",
    "I had to have this talk with you\n",
    "My happiness depends on you\n",
    "And whatever you decide to do, Jolene\n",
    "\n",
    "Jolene, Jolene, Jolene, Jolene\n",
    "I'm begging of you please don't take my man\n",
    "Jolene, Jolene, Jolene, Jolene\n",
    "Please don't take him even though you can\n",
    "\n",
    "Jolene, Jolene\"\"\"\n",
    "\n",
    "\n",
    "**Louis Armstrong - What a wonderful world**\n",
    "\n",
    "\"\"\"I see trees of green\n",
    "Red roses too\n",
    "I see them bloom\n",
    "For me and you\n",
    "And I think to myself\n",
    "What a wonderful world\n",
    "\n",
    "I see skies of blue\n",
    "And clouds of white\n",
    "The bright blessed day\n",
    "The dark sacred night\n",
    "And I think to myself\n",
    "What a wonderful world\n",
    "\n",
    "The colors of the rainbow\n",
    "So pretty in the sky\n",
    "Are also on the faces\n",
    "Of people going by\n",
    "I see friends shaking hands\n",
    "Saying, \"How do you do?\"\n",
    "They're really saying\n",
    "\"I love you\"\n",
    "\n",
    "I hear babies cry\n",
    "I watch them grow\n",
    "They'll learn much more\n",
    "Than I'll never know\n",
    "And I think to myself\n",
    "What a wonderful world\n",
    "\n",
    "Yes, I think to myself\n",
    "What a wonderful world\n",
    "\n",
    "Oh yeah\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**Rage against the Machine - Killing in the Name**\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Killing in the name of\n",
    "Some of those that work forces, are the same that burn crosses\n",
    "Some of those that work forces, are the same that burn crosses\n",
    "Some of those that work forces, are the same that burn crosses\n",
    "Some of those that work forces, are the same that burn crosses\n",
    "Huh!\n",
    "\n",
    "Killing in the name of\n",
    "Killing in the name of\n",
    "\n",
    "And now you do what they told ya\n",
    "And now you do what they told ya\n",
    "And now you do what they told ya\n",
    "And now you do what they told ya\n",
    "And now you do what they told ya\n",
    "And now you do what they told ya\n",
    "And now you do what they told ya\n",
    "But now you do what they told ya\n",
    "Well now you do what they told ya\n",
    "\n",
    "Those who died are justified, for wearing the badge, they're the chosen whites\n",
    "You justify those that died by wearing the badge, they're the chosen whites\n",
    "Those who died are justified, for wearing the badge, they're the chosen whites\n",
    "You justify those that died by wearing the badge, they're the chosen whites\n",
    "\n",
    "Some of those that work forces, are the same that burn crosses\n",
    "Some of those that work forces, are the same that burn crosses\n",
    "Some of those that work forces, are the same that burn crosses\n",
    "Some of those that work forces, are the same that burn crosses\n",
    "Uggh!\n",
    "\n",
    "Killing in the name of\n",
    "Killing in the name of\n",
    "\n",
    "\n",
    "Those who died are justified, for wearing the badge, they're the chosen whites\n",
    "You justify those that died by wearing the badge, they're the chosen whites\n",
    "Those who died are justified, for wearing the badge, they're the chosen whites\n",
    "You justify those that died by wearing the badge, they're the chosen whites\n",
    "Come on!\n",
    "\n",
    "Yeah! Come on!\n",
    "\n",
    "Fuck you, I won't do what you tell me\n",
    "Fuck you, I won't do what you tell me\n",
    "Fuck you, I won't do what you tell me\n",
    "Fuck you, I won't do what you tell me\n",
    "Fuck you, I won't do what you tell me\n",
    "Fuck you, I won't do what you tell me\n",
    "Fuck you, I won't do what you tell me\n",
    "Fuck you, I won't do what you tell me\n",
    "\n",
    "Motherfucker!\n",
    "Uggh!\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "**Three Days Grace - I hate everything about you**\n",
    "\n",
    "\n",
    "\"\"\"Every time we lie awake\n",
    "After every hit we take\n",
    "Every feeling that I get\n",
    "But I haven't missed you yet\n",
    "\n",
    "Every roommate kept awake\n",
    "By every sigh and scream we make\n",
    "All the feelings that I get\n",
    "But I still don't miss you yet\n",
    "\n",
    "Only when I stop to think about it\n",
    "\n",
    "I hate everything about you\n",
    "Why do I love you?\n",
    "I hate everything about you\n",
    "Why do I love you?\n",
    "\n",
    "Every time we lie awake\n",
    "After every hit we take\n",
    "Every feeling that I get\n",
    "But I haven't missed you yet\n",
    "\n",
    "Only when I stop to think about it\n",
    "\n",
    "I hate everything about you\n",
    "Why do I love you?\n",
    "I hate everything about you\n",
    "Why do I love you?\n",
    "\n",
    "Only when I stop to think about you\n",
    "I know\n",
    "Only when you stop to think about me\n",
    "Do you know?\n",
    "\n",
    "I hate everything about you\n",
    "Why do I love you?\n",
    "You hate everything about me\n",
    "Why do you love me?\n",
    "\n",
    "I hate\n",
    "You hate\n",
    "I hate\n",
    "You love me\n",
    "\n",
    "I hate everything about you\n",
    "Why do I love you?\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLPVENV2",
   "language": "python",
   "name": "nlpvenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
