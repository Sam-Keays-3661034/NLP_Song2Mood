{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a2076ca2-ed69-4f07-8546-b53646929df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from datasets import Dataset\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e150b3ce-b48e-49c6-8a15-9633a3ebb2ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\".*pin_memory.*\")\n",
    "\n",
    "from transformers import logging as hf_logging\n",
    "hf_logging.set_verbosity_error()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a173a804-807a-4752-8c1f-8b7eb3ec60e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "############################################################\n",
    "# 1. Setup and constants\n",
    "############################################################\n",
    "\n",
    "EMOTIONS = [\"Angry\", \"Happy\", \"Relaxed\", \"Sad\"]\n",
    "\n",
    "label2id = {label: i for i, label in enumerate(EMOTIONS)}\n",
    "id2label = {i: label for label, i in label2id.items()}\n",
    "\n",
    "# Timestamp pattern like [01:23.45]\n",
    "timestamp_pattern = re.compile(r\"\\[\\d{2}:\\d{2}(?:\\.\\d{2})?\\]\")\n",
    "\n",
    "DATASET_DIR = \"NJU_MusicMood_v1.0\"\n",
    "\n",
    "############################################################\n",
    "# 2. Cleaning function (clean text for modeling)\n",
    "############################################################\n",
    "\n",
    "def clean_lyrics(text: str) -> str:\n",
    "    # Remove timestamps like [00:29]\n",
    "    text = timestamp_pattern.sub(\"\", text)\n",
    "\n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Normalize quotes\n",
    "    text = text.replace(\"’\", \"'\").replace(\"“\", '\"').replace(\"”\", '\"')\n",
    "\n",
    "    # Remove ellipses and repeated dots\n",
    "    text = re.sub(r\"\\.{2,}\", \" \", text)\n",
    "\n",
    "    # Remove long underscores\n",
    "    text = re.sub(r\"_{2,}\", \" \", text)\n",
    "\n",
    "    # Remove trailing \"end\" markers\n",
    "    text = re.sub(r\"\\bend[.\\s]*$\", \"\", text.strip())\n",
    "\n",
    "    # # Replace newlines with space\n",
    "    # text = text.replace(\"\\n\", \" \")\n",
    "\n",
    "    # Remove special characters except letters, digits, spaces, apostrophes\n",
    "    text = re.sub(r\"[^a-z0-9' ]+\", \" \", text)\n",
    "\n",
    "    # Collapse multiple spaces\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "############################################################\n",
    "# 3. Load dataset\n",
    "############################################################\n",
    "\n",
    "def get_lyrics(path: str) -> str:\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        raw = f.read()\n",
    "    return clean_lyrics(raw)\n",
    "\n",
    "def get_lyrics_and_labels(split: str):\n",
    "    texts, labels = [], []\n",
    "    for emotion in EMOTIONS:\n",
    "        folder = os.path.join(DATASET_DIR, emotion, split)\n",
    "\n",
    "        if not os.path.isdir(folder):\n",
    "            continue\n",
    "\n",
    "        for fname in os.listdir(folder):\n",
    "            if fname.lower() == \"info.txt\":\n",
    "                continue\n",
    "            if not fname.endswith(\".txt\"):\n",
    "                continue\n",
    "\n",
    "            path = os.path.join(folder, fname)\n",
    "            text = get_lyrics(path)\n",
    "\n",
    "            if text.strip():\n",
    "                texts.append(text)\n",
    "                labels.append(emotion)\n",
    "\n",
    "    return texts, labels\n",
    "\n",
    "train_texts, train_labels = get_lyrics_and_labels(\"Train\")\n",
    "dev_texts, dev_labels = get_lyrics_and_labels(\"Test\")\n",
    "\n",
    "train_ds = Dataset.from_dict({\n",
    "    \"text\": train_texts,\n",
    "    \"label\": [label2id[l] for l in train_labels]\n",
    "})\n",
    "\n",
    "dev_ds = Dataset.from_dict({\n",
    "    \"text\": dev_texts,\n",
    "    \"label\": [label2id[l] for l in dev_labels]\n",
    "})\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f511c087-4047-4c1f-be5c-1e4c865c28be",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################\n",
    "# 4. Helper for evaluation\n",
    "############################################################\n",
    "\n",
    "def print_results(true_labels, predicted_labels):\n",
    "    p, r, f, _ = precision_recall_fscore_support(\n",
    "        true_labels, predicted_labels, average=\"macro\", zero_division=0\n",
    "    )\n",
    "    acc = accuracy_score(true_labels, predicted_labels)\n",
    "\n",
    "    print(\"Macro Precision:\", p)\n",
    "    print(\"Macro Recall:\", r)\n",
    "    print(\"Macro F1:\", f)\n",
    "    print(\"Accuracy:\", acc)\n",
    "\n",
    "\n",
    "############################################################\n",
    "# 5. Position tagging function (START, MID, END)\n",
    "############################################################\n",
    "\n",
    "def position_tag(text):\n",
    "    \"\"\"Tag each token with its position in the song.\n",
    "       First 20 percent = _START\n",
    "       Middle 60 percent = _MID\n",
    "       Last 20 percent = _END\n",
    "    \"\"\"\n",
    "    tokens = text.split()\n",
    "    n = len(tokens)\n",
    "    tagged = []\n",
    "\n",
    "    for i, tok in enumerate(tokens):\n",
    "        ratio = i / n\n",
    "        if ratio < 0.2:\n",
    "            tagged.append(tok + \"_START\")\n",
    "        elif ratio > 0.8:\n",
    "            tagged.append(tok + \"_END\")\n",
    "        else:\n",
    "            tagged.append(tok + \"_MID\")\n",
    "\n",
    "    return \" \".join(tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c69e42fe-5779-482a-b97d-528ffc3cb055",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "\n",
    "def print_results(gold_labels, predicted_labels):\n",
    "    # Overall macro metrics\n",
    "    p, r, f, _ = precision_recall_fscore_support(\n",
    "        gold_labels, predicted_labels, average=\"macro\", zero_division=0\n",
    "    )\n",
    "    acc = accuracy_score(gold_labels, predicted_labels)\n",
    "\n",
    "    print(\"=== Overall (Macro Avg) ===\")\n",
    "    print(\"Precision:\", p)\n",
    "    print(\"Recall:\", r)\n",
    "    print(\"F1:\", f)\n",
    "    print(\"Accuracy:\", acc)\n",
    "    print()\n",
    "\n",
    "    # Per class metrics\n",
    "    p_i, r_i, f_i, _ = precision_recall_fscore_support(\n",
    "        gold_labels, predicted_labels, average=None, zero_division=0\n",
    "    )\n",
    "\n",
    "    print(\"=== Per Emotion (Class) Metrics ===\")\n",
    "    for i, emotion in enumerate(EMOTIONS):\n",
    "        print(f\"{emotion}:\")\n",
    "        print(\"  Precision:\", p_i[i])\n",
    "        print(\"  Recall:   \", r_i[i])\n",
    "        print(\"  F1:       \", f_i[i])\n",
    "    print()  # empty line at the end\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a44ac8-ce4a-4ae9-8265-6647dba87464",
   "metadata": {},
   "outputs": [],
   "source": [
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2f0c9ef1-51a3-45f5-8c59-05ca82663e99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Training Logistic Regression with Position Tags (START/MID/END) ===\n",
      "\n",
      "=== Results with START/MID/END position tagging ===\n",
      "=== Overall (Macro Avg) ===\n",
      "Precision: 0.3902636986855047\n",
      "Recall: 0.40970226440661606\n",
      "F1: 0.39418263460750874\n",
      "Accuracy: 0.38992042440318303\n",
      "\n",
      "=== Per Emotion (Class) Metrics ===\n",
      "Angry:\n",
      "  Precision: 0.45544554455445546\n",
      "  Recall:    0.647887323943662\n",
      "  F1:        0.5348837209302325\n",
      "Happy:\n",
      "  Precision: 0.40963855421686746\n",
      "  Recall:    0.32075471698113206\n",
      "  F1:        0.35978835978835977\n",
      "Relaxed:\n",
      "  Precision: 0.3626373626373626\n",
      "  Recall:    0.32673267326732675\n",
      "  F1:        0.34375\n",
      "Sad:\n",
      "  Precision: 0.3333333333333333\n",
      "  Recall:    0.3434343434343434\n",
      "  F1:        0.3383084577114428\n",
      "\n"
     ]
    }
   ],
   "source": [
    "############################################################\n",
    "# 6. Vectorize with TF-IDF + train Logistic Regression\n",
    "############################################################\n",
    "\n",
    "print(\"=== Training Logistic Regression with Position Tags (START/MID/END) ===\")\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "    max_features=50000,\n",
    "    ngram_range=(1, 2)           # unigrams + bigrams improve performance\n",
    ")\n",
    "train_tagged = [position_tag(t) for t in train_texts]\n",
    "dev_tagged = [position_tag(t) for t in dev_texts]\n",
    "X_train = vectorizer.fit_transform(train_tagged)\n",
    "X_dev = vectorizer.transform(dev_tagged)\n",
    "\n",
    "clf = LogisticRegression(\n",
    "    max_iter=2000,\n",
    "    class_weight=\"balanced\",\n",
    "    # multi_class=\"multinomial\",\n",
    "    solver=\"lbfgs\"\n",
    ")\n",
    "\n",
    "clf.fit(X_train, train_ds[\"label\"])\n",
    "\n",
    "preds = clf.predict(X_dev)\n",
    "\n",
    "############################################################\n",
    "# 7. Print results\n",
    "############################################################\n",
    "\n",
    "print(\"\\n=== Results with START/MID/END position tagging ===\")\n",
    "print_results(dev_ds[\"label\"], preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6e091914-4e73-4598-af18-16d4fbf63b58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Training Logistic Regression with Position Tags (START/MID/END) ===\n",
      "\n",
      "=== Emotion based results with START/MID/END position tagging ===\n",
      "=== Overall (Macro Avg) ===\n",
      "Precision: 0.3902636986855047\n",
      "Recall: 0.40970226440661606\n",
      "F1: 0.39418263460750874\n",
      "Accuracy: 0.38992042440318303\n",
      "\n",
      "=== Per Emotion (Class) Metrics ===\n",
      "Angry:\n",
      "  Precision: 0.45544554455445546\n",
      "  Recall:    0.647887323943662\n",
      "  F1:        0.5348837209302325\n",
      "Happy:\n",
      "  Precision: 0.40963855421686746\n",
      "  Recall:    0.32075471698113206\n",
      "  F1:        0.35978835978835977\n",
      "Relaxed:\n",
      "  Precision: 0.3626373626373626\n",
      "  Recall:    0.32673267326732675\n",
      "  F1:        0.34375\n",
      "Sad:\n",
      "  Precision: 0.3333333333333333\n",
      "  Recall:    0.3434343434343434\n",
      "  F1:        0.3383084577114428\n",
      "\n"
     ]
    }
   ],
   "source": [
    "############################################################\n",
    "# 6. Vectorize with TF-IDF + train Logistic Regression\n",
    "############################################################\n",
    "\n",
    "print(\"=== Training Logistic Regression with Position Tags (START/MID/END) ===\")\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "    max_features=50000,\n",
    "    ngram_range=(1, 2)           # unigrams + bigrams improve performance\n",
    ")\n",
    "\n",
    "X_train = vectorizer.fit_transform(train_tagged)\n",
    "X_dev = vectorizer.transform(dev_tagged)\n",
    "\n",
    "clf = LogisticRegression(\n",
    "    max_iter=2000,\n",
    "    class_weight=\"balanced\",\n",
    "    # multi_class=\"multinomial\",\n",
    "    solver=\"lbfgs\"\n",
    ")\n",
    "\n",
    "clf.fit(X_train, train_ds[\"label\"])\n",
    "\n",
    "preds = clf.predict(X_dev)\n",
    "\n",
    "############################################################\n",
    "# 7. Print emotion based results\n",
    "############################################################\n",
    "\n",
    "print(\"\\n=== Emotion based results with START/MID/END position tagging ===\")\n",
    "print_results(dev_ds[\"label\"], preds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74db0f2e-b107-4e50-a8c3-8fd84c7f72c6",
   "metadata": {},
   "source": [
    "## Separate Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "edae0849-c024-472c-b012-0dfdfe580eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_segment(text, segment=\"start\", portion=0.3):\n",
    "    \"\"\"\n",
    "    Extracts a portion of the lyrics.\n",
    "    portion=0.3 means 30 percent of lyrics.\n",
    "    segment can be start, middle, or end.\n",
    "    \"\"\"\n",
    "    tokens = text.split()\n",
    "    n = len(tokens)\n",
    "\n",
    "    if n == 0:\n",
    "        return \"\"\n",
    "\n",
    "    cut = int(n * portion)  # number of tokens per section\n",
    "\n",
    "    if segment == \"start\":\n",
    "        return \" \".join(tokens[:cut])\n",
    "\n",
    "    elif segment == \"middle\":\n",
    "        start = int(n * 0.35)\n",
    "        end = int(n * 0.65)\n",
    "        return \" \".join(tokens[start:end])\n",
    "\n",
    "    elif segment == \"end\":\n",
    "        return \" \".join(tokens[-cut:])\n",
    "\n",
    "    else:\n",
    "        return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "36360769-eae1-4899-ae40-fb0ba0e73780",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build segmented datasets\n",
    "train_start = [get_segment(t, \"start\") for t in train_ds[\"text\"]]\n",
    "train_middle = [get_segment(t, \"middle\") for t in train_ds[\"text\"]]\n",
    "train_end = [get_segment(t, \"end\") for t in train_ds[\"text\"]]\n",
    "\n",
    "dev_start = [get_segment(t, \"start\") for t in dev_ds[\"text\"]]\n",
    "dev_middle = [get_segment(t, \"middle\") for t in dev_ds[\"text\"]]\n",
    "dev_end = [get_segment(t, \"end\") for t in dev_ds[\"text\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0b1447b2-d63e-4047-912c-176844a7ff18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_segment_model(train_texts, dev_texts, train_labels, dev_labels, name=\"\"):\n",
    "    print(f\"\\n=== Training {name} model ===\")\n",
    "\n",
    "    vectorizer = TfidfVectorizer(\n",
    "        max_features=50000,\n",
    "        ngram_range=(1, 2)\n",
    "    )\n",
    "\n",
    "    X_train = vectorizer.fit_transform(train_texts)\n",
    "    X_dev = vectorizer.transform(dev_texts)\n",
    "\n",
    "    clf = LogisticRegression(\n",
    "        max_iter=2000,\n",
    "        class_weight=\"balanced\",\n",
    "        # multi_class=\"multinomial\"\n",
    "    )\n",
    "\n",
    "    clf.fit(X_train, train_labels)\n",
    "    preds = clf.predict(X_dev)\n",
    "\n",
    "    print(f\"=== Results ({name}) ===\")\n",
    "    print_results(dev_labels, preds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4b564995-3f4b-4218-975d-3a070cea935d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Training BEGINNING ONLY model ===\n",
      "=== Results (BEGINNING ONLY) ===\n",
      "=== Overall (Macro Avg) ===\n",
      "Precision: 0.3458269076705815\n",
      "Recall: 0.3602639861381833\n",
      "F1: 0.3481884320377146\n",
      "Accuracy: 0.3421750663129973\n",
      "\n",
      "=== Per Emotion (Class) Metrics ===\n",
      "Angry:\n",
      "  Precision: 0.4375\n",
      "  Recall:    0.5915492957746479\n",
      "  F1:        0.5029940119760479\n",
      "Happy:\n",
      "  Precision: 0.4157303370786517\n",
      "  Recall:    0.3490566037735849\n",
      "  F1:        0.37948717948717947\n",
      "Relaxed:\n",
      "  Precision: 0.2911392405063291\n",
      "  Recall:    0.22772277227722773\n",
      "  F1:        0.25555555555555554\n",
      "Sad:\n",
      "  Precision: 0.23893805309734514\n",
      "  Recall:    0.2727272727272727\n",
      "  F1:        0.25471698113207547\n",
      "\n",
      "\n",
      "=== Training MIDDLE ONLY model ===\n",
      "=== Results (MIDDLE ONLY) ===\n",
      "=== Overall (Macro Avg) ===\n",
      "Precision: 0.4168987689585285\n",
      "Recall: 0.43011603844413143\n",
      "F1: 0.4214992552389918\n",
      "Accuracy: 0.41644562334217505\n",
      "\n",
      "=== Per Emotion (Class) Metrics ===\n",
      "Angry:\n",
      "  Precision: 0.48863636363636365\n",
      "  Recall:    0.6056338028169014\n",
      "  F1:        0.5408805031446541\n",
      "Happy:\n",
      "  Precision: 0.46938775510204084\n",
      "  Recall:    0.4339622641509434\n",
      "  F1:        0.45098039215686275\n",
      "Relaxed:\n",
      "  Precision: 0.3333333333333333\n",
      "  Recall:    0.297029702970297\n",
      "  F1:        0.31413612565445026\n",
      "Sad:\n",
      "  Precision: 0.37623762376237624\n",
      "  Recall:    0.3838383838383838\n",
      "  F1:        0.38\n",
      "\n",
      "\n",
      "=== Training END ONLY model ===\n",
      "=== Results (END ONLY) ===\n",
      "=== Overall (Macro Avg) ===\n",
      "Precision: 0.34643808610400684\n",
      "Recall: 0.36406859195087726\n",
      "F1: 0.3458654058052186\n",
      "Accuracy: 0.34748010610079577\n",
      "\n",
      "=== Per Emotion (Class) Metrics ===\n",
      "Angry:\n",
      "  Precision: 0.3416666666666667\n",
      "  Recall:    0.5774647887323944\n",
      "  F1:        0.4293193717277487\n",
      "Happy:\n",
      "  Precision: 0.40217391304347827\n",
      "  Recall:    0.3490566037735849\n",
      "  F1:        0.37373737373737376\n",
      "Relaxed:\n",
      "  Precision: 0.32941176470588235\n",
      "  Recall:    0.27722772277227725\n",
      "  F1:        0.3010752688172043\n",
      "Sad:\n",
      "  Precision: 0.3125\n",
      "  Recall:    0.25252525252525254\n",
      "  F1:        0.27932960893854747\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train and evaluate beginning model\n",
    "train_segment_model(\n",
    "    train_start,\n",
    "    dev_start,\n",
    "    train_ds[\"label\"],\n",
    "    dev_ds[\"label\"],\n",
    "    name=\"BEGINNING ONLY\"\n",
    ")\n",
    "\n",
    "# Train and evaluate middle model\n",
    "train_segment_model(\n",
    "    train_middle,\n",
    "    dev_middle,\n",
    "    train_ds[\"label\"],\n",
    "    dev_ds[\"label\"],\n",
    "    name=\"MIDDLE ONLY\"\n",
    ")\n",
    "\n",
    "# Train and evaluate end model\n",
    "train_segment_model(\n",
    "    train_end,\n",
    "    dev_end,\n",
    "    train_ds[\"label\"],\n",
    "    dev_ds[\"label\"],\n",
    "    name=\"END ONLY\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f9bae9",
   "metadata": {},
   "source": [
    "## DistilBERT with position tags\n",
    "Fine-tune a transformer on position-tagged lyrics so the model can learn positional cues alongside word content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "be3e51c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, Trainer, TrainingArguments, DataCollatorWithPadding\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\".*pin_memory.*\")\n",
    "\n",
    "from transformers import logging\n",
    "logging.set_verbosity_error()\n",
    "\n",
    "\n",
    "\n",
    "def distilbert_tokenize(dataset, tokenizer, text_field=\"text\", max_length=256):\n",
    "    \"Tokenize a dataset column and set torch format for Trainer.\"\n",
    "    def _tok(batch):\n",
    "        return tokenizer(batch[text_field], truncation=True, padding=\"max_length\", max_length=max_length)\n",
    "\n",
    "    tokenized = dataset.map(_tok, batched=True)\n",
    "    tokenized = tokenized.remove_columns([text_field])\n",
    "    tokenized.set_format(type=\"torch\")\n",
    "    return tokenized\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    p, r, f, _ = precision_recall_fscore_support(labels, preds, average=\"macro\", zero_division=0)\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\"precision\": p, \"recall\": r, \"f1\": f, \"accuracy\": acc}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a6d44ead",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 400/400 [00:00<00:00, 7952.91 examples/s]\n",
      "Map: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 377/377 [00:00<00:00, 13835.44 examples/s]\n",
      "Map: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 400/400 [00:00<00:00, 3799.65 examples/s]\n",
      "Map: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 377/377 [00:00<00:00, 4098.89 examples/s]\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert/distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/Users/sultanarazia/miniconda3/envs/nlp_env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='300' max='300' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [300/300 02:13, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.392600</td>\n",
       "      <td>1.391100</td>\n",
       "      <td>0.047082</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.079241</td>\n",
       "      <td>0.188329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.396800</td>\n",
       "      <td>1.383823</td>\n",
       "      <td>0.143604</td>\n",
       "      <td>0.283778</td>\n",
       "      <td>0.189136</td>\n",
       "      <td>0.302387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.386600</td>\n",
       "      <td>1.382379</td>\n",
       "      <td>0.327411</td>\n",
       "      <td>0.300948</td>\n",
       "      <td>0.216044</td>\n",
       "      <td>0.328912</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sultanarazia/miniconda3/envs/nlp_env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/sultanarazia/miniconda3/envs/nlp_env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/sultanarazia/miniconda3/envs/nlp_env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sultanarazia/miniconda3/envs/nlp_env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== DistilBERT (position-tagged lyrics) ===\n",
      "=== Overall (Macro Avg) ===\n",
      "Precision: 0.32741141732283463\n",
      "Recall: 0.30094829506867776\n",
      "F1: 0.21604395491381792\n",
      "Accuracy: 0.32891246684350134\n",
      "\n",
      "=== Per Emotion (Class) Metrics ===\n",
      "Angry:\n",
      "  Precision: 0.6666666666666666\n",
      "  Recall:    0.028169014084507043\n",
      "  F1:        0.05405405405405406\n",
      "Happy:\n",
      "  Precision: 0.3346456692913386\n",
      "  Recall:    0.8018867924528302\n",
      "  F1:        0.4722222222222222\n",
      "Relaxed:\n",
      "  Precision: 0.0\n",
      "  Recall:    0.0\n",
      "  F1:        0.0\n",
      "Sad:\n",
      "  Precision: 0.30833333333333335\n",
      "  Recall:    0.37373737373737376\n",
      "  F1:        0.3378995433789954\n",
      "\n",
      "Trainer eval metrics: {'eval_loss': 1.382379412651062, 'eval_precision': 0.32741141732283463, 'eval_recall': 0.30094829506867776, 'eval_f1': 0.21604395491381792, 'eval_accuracy': 0.32891246684350134, 'eval_runtime': 9.3576, 'eval_samples_per_second': 40.288, 'eval_steps_per_second': 10.152, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "# Apply START/MID/END tags before feeding lyrics into DistilBERT\n",
    "position_train_ds = train_ds.map(lambda ex: {\"text\": position_tag(ex[\"text\"])})\n",
    "position_dev_ds = dev_ds.map(lambda ex: {\"text\": position_tag(ex[\"text\"])})\n",
    "\n",
    "model_name = \"distilbert/distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "tokenized_train = distilbert_tokenize(position_train_ds, tokenizer)\n",
    "tokenized_dev = distilbert_tokenize(position_dev_ds, tokenizer)\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=len(EMOTIONS),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    "    ignore_mismatched_sizes=True,\n",
    ")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"distilbert_position_output\",\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    weight_decay=0.01,\n",
    "    eval_strategy=\"epoch\",\n",
    "    logging_steps=25,\n",
    "    save_strategy=\"epoch\",\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_dev,\n",
    "    compute_metrics=compute_metrics,\n",
    "    processing_class=tokenizer,\n",
    ")\n",
    "\n",
    "# Fine-tune DistilBERT on position-tagged lyrics\n",
    "trainer.train()\n",
    "eval_results = trainer.evaluate()\n",
    "\n",
    "# Get predictions on the dev split to reuse the earlier reporting helper\n",
    "position_logits = trainer.predict(tokenized_dev).predictions\n",
    "position_preds = np.argmax(position_logits, axis=-1)\n",
    "\n",
    "print(\"\\n=== DistilBERT (position-tagged lyrics) ===\")\n",
    "print_results(dev_ds[\"label\"], position_preds)\n",
    "print(\"Trainer eval metrics:\", eval_results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ed06e5-6041-4ddd-8fdc-d00f5db9e953",
   "metadata": {},
   "source": [
    "## Build DistilBERT-ready segmented datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a0f284a3-de93-42ba-be25-896cd9876168",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "# Build segmented versions\n",
    "train_start_ds = Dataset.from_dict({\n",
    "    \"text\": [get_segment(t, \"start\") for t in train_ds[\"text\"]],\n",
    "    \"label\": train_ds[\"label\"]\n",
    "})\n",
    "\n",
    "train_middle_ds = Dataset.from_dict({\n",
    "    \"text\": [get_segment(t, \"middle\") for t in train_ds[\"text\"]],\n",
    "    \"label\": train_ds[\"label\"]\n",
    "})\n",
    "\n",
    "train_end_ds = Dataset.from_dict({\n",
    "    \"text\": [get_segment(t, \"end\") for t in train_ds[\"text\"]],\n",
    "    \"label\": train_ds[\"label\"]\n",
    "})\n",
    "\n",
    "dev_start_ds = Dataset.from_dict({\n",
    "    \"text\": [get_segment(t, \"start\") for t in dev_ds[\"text\"]],\n",
    "    \"label\": dev_ds[\"label\"]\n",
    "})\n",
    "\n",
    "dev_middle_ds = Dataset.from_dict({\n",
    "    \"text\": [get_segment(t, \"middle\") for t in dev_ds[\"text\"]],\n",
    "    \"label\": dev_ds[\"label\"]\n",
    "})\n",
    "\n",
    "dev_end_ds = Dataset.from_dict({\n",
    "    \"text\": [get_segment(t, \"end\") for t in dev_ds[\"text\"]],\n",
    "    \"label\": dev_ds[\"label\"]\n",
    "})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7434f1e8-1572-4bb2-9de5-cbd5718b5b78",
   "metadata": {},
   "source": [
    "## DistilBERT training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e3033e12-1407-4161-b06f-bfbcdd6c3975",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_distilbert_segment(train_ds, dev_ds, label2id, id2label, name=\"\"):\n",
    "    print(f\"\\n=== Training DistilBERT ({name}) ===\")\n",
    "\n",
    "    model_name = \"distilbert-base-uncased\"\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    # Tokenizer mapping\n",
    "    def tokenize_batch(batch):\n",
    "        return tokenizer(\n",
    "            batch[\"text\"],\n",
    "            truncation=True,\n",
    "            padding=False,\n",
    "            max_length=256\n",
    "        )\n",
    "\n",
    "    tokenized_train = train_ds.map(tokenize_batch, batched=True)\n",
    "    tokenized_dev = dev_ds.map(tokenize_batch, batched=True)\n",
    "\n",
    "    tokenized_train = tokenized_train.remove_columns([\"text\"])\n",
    "    tokenized_dev = tokenized_dev.remove_columns([\"text\"])\n",
    "    tokenized_train.set_format(\"torch\")\n",
    "    tokenized_dev.set_format(\"torch\")\n",
    "\n",
    "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_name,\n",
    "        num_labels=len(EMOTIONS),\n",
    "        id2label=id2label,\n",
    "        label2id=label2id\n",
    "    )\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=f\"distilbert_{name.lower().replace(' ','_')}\",\n",
    "        num_train_epochs=3,\n",
    "        learning_rate=5e-5,\n",
    "        per_device_train_batch_size=4,\n",
    "        per_device_eval_batch_size=4,\n",
    "        weight_decay=0.01,\n",
    "        eval_strategy=\"epoch\",          \n",
    "        save_strategy=\"epoch\",\n",
    "        logging_steps=25,\n",
    "        report_to=\"none\"\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_train,\n",
    "        eval_dataset=tokenized_dev,\n",
    "        data_collator=data_collator,\n",
    "        processing_class=tokenizer,\n",
    "        compute_metrics=compute_metrics\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "    pred_output = trainer.predict(tokenized_dev)\n",
    "    pred_ids = np.argmax(pred_output.predictions, axis=-1)\n",
    "\n",
    "    print(f\"\\n=== Results for DistilBERT ({name}) ===\")\n",
    "    print_results(dev_ds[\"label\"], pred_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "87762d61-ed4f-42d4-97a7-733a1bf8c27e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Training DistilBERT (BEGINNING ONLY) ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 400/400 [00:00<00:00, 16225.86 examples/s]\n",
      "Map: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 377/377 [00:00<00:00, 21070.73 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3862, 'grad_norm': 8.852052688598633, 'learning_rate': 4.600000000000001e-05, 'epoch': 0.25}\n",
      "{'loss': 1.4156, 'grad_norm': 4.586274147033691, 'learning_rate': 4.183333333333334e-05, 'epoch': 0.5}\n",
      "{'loss': 1.4111, 'grad_norm': 3.341245651245117, 'learning_rate': 3.766666666666667e-05, 'epoch': 0.75}\n",
      "{'loss': 1.41, 'grad_norm': 3.914538860321045, 'learning_rate': 3.35e-05, 'epoch': 1.0}\n",
      "{'eval_loss': 1.349113941192627, 'eval_precision': 0.1911770680968633, 'eval_recall': 0.3654302049923302, 'eval_f1': 0.23991321434419519, 'eval_accuracy': 0.35013262599469497, 'eval_runtime': 7.6282, 'eval_samples_per_second': 49.422, 'eval_steps_per_second': 12.454, 'epoch': 1.0}\n",
      "{'loss': 1.29, 'grad_norm': 4.489414691925049, 'learning_rate': 2.9333333333333336e-05, 'epoch': 1.25}\n",
      "{'loss': 1.2563, 'grad_norm': 8.825271606445312, 'learning_rate': 2.5166666666666667e-05, 'epoch': 1.5}\n",
      "{'loss': 1.3849, 'grad_norm': 4.975724220275879, 'learning_rate': 2.1e-05, 'epoch': 1.75}\n",
      "{'loss': 1.132, 'grad_norm': 5.535017490386963, 'learning_rate': 1.6833333333333334e-05, 'epoch': 2.0}\n",
      "{'eval_loss': 1.210563063621521, 'eval_precision': 0.4893393005461971, 'eval_recall': 0.47367430737227323, 'eval_f1': 0.43439399246600174, 'eval_accuracy': 0.4535809018567639, 'eval_runtime': 5.582, 'eval_samples_per_second': 67.539, 'eval_steps_per_second': 17.019, 'epoch': 2.0}\n",
      "{'loss': 0.9175, 'grad_norm': 4.359683990478516, 'learning_rate': 1.2666666666666668e-05, 'epoch': 2.25}\n",
      "{'loss': 0.9718, 'grad_norm': 8.004459381103516, 'learning_rate': 8.500000000000002e-06, 'epoch': 2.5}\n",
      "{'loss': 1.0101, 'grad_norm': 3.0318996906280518, 'learning_rate': 4.333333333333334e-06, 'epoch': 2.75}\n",
      "{'loss': 0.9898, 'grad_norm': 8.7536039352417, 'learning_rate': 1.6666666666666668e-07, 'epoch': 3.0}\n",
      "{'eval_loss': 1.1848230361938477, 'eval_precision': 0.46541955293325904, 'eval_recall': 0.47239072831014167, 'eval_f1': 0.4614809900161368, 'eval_accuracy': 0.4509283819628647, 'eval_runtime': 5.4045, 'eval_samples_per_second': 69.757, 'eval_steps_per_second': 17.578, 'epoch': 3.0}\n",
      "{'train_runtime': 98.9514, 'train_samples_per_second': 12.127, 'train_steps_per_second': 3.032, 'train_loss': 1.2146034622192383, 'epoch': 3.0}\n",
      "\n",
      "=== Results for DistilBERT (BEGINNING ONLY) ===\n",
      "=== Overall (Macro Avg) ===\n",
      "Precision: 0.46541955293325904\n",
      "Recall: 0.47239072831014167\n",
      "F1: 0.4614809900161368\n",
      "Accuracy: 0.4509283819628647\n",
      "\n",
      "=== Per Emotion (Class) Metrics ===\n",
      "Angry:\n",
      "  Precision: 0.6419753086419753\n",
      "  Recall:    0.7323943661971831\n",
      "  F1:        0.6842105263157895\n",
      "Happy:\n",
      "  Precision: 0.4444444444444444\n",
      "  Recall:    0.41509433962264153\n",
      "  F1:        0.4292682926829268\n",
      "Relaxed:\n",
      "  Precision: 0.421875\n",
      "  Recall:    0.26732673267326734\n",
      "  F1:        0.32727272727272727\n",
      "Sad:\n",
      "  Precision: 0.3533834586466165\n",
      "  Recall:    0.47474747474747475\n",
      "  F1:        0.4051724137931034\n",
      "\n",
      "\n",
      "=== Training DistilBERT (MIDDLE ONLY) ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 400/400 [00:00<00:00, 11068.08 examples/s]\n",
      "Map: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 377/377 [00:00<00:00, 15053.38 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4118, 'grad_norm': 4.958386421203613, 'learning_rate': 4.600000000000001e-05, 'epoch': 0.25}\n",
      "{'loss': 1.3816, 'grad_norm': 2.928382158279419, 'learning_rate': 4.183333333333334e-05, 'epoch': 0.5}\n",
      "{'loss': 1.3188, 'grad_norm': 5.535400867462158, 'learning_rate': 3.766666666666667e-05, 'epoch': 0.75}\n",
      "{'loss': 1.2922, 'grad_norm': 7.746114253997803, 'learning_rate': 3.35e-05, 'epoch': 1.0}\n",
      "{'eval_loss': 1.2121134996414185, 'eval_precision': 0.4825542717086835, 'eval_recall': 0.4684201445656126, 'eval_f1': 0.4316324829269062, 'eval_accuracy': 0.4562334217506631, 'eval_runtime': 5.7614, 'eval_samples_per_second': 65.435, 'eval_steps_per_second': 16.489, 'epoch': 1.0}\n",
      "{'loss': 1.0936, 'grad_norm': 4.896649360656738, 'learning_rate': 2.9333333333333336e-05, 'epoch': 1.25}\n",
      "{'loss': 0.9986, 'grad_norm': 6.769309043884277, 'learning_rate': 2.5166666666666667e-05, 'epoch': 1.5}\n",
      "{'loss': 1.1846, 'grad_norm': 8.552339553833008, 'learning_rate': 2.1e-05, 'epoch': 1.75}\n",
      "{'loss': 0.8799, 'grad_norm': 10.83148193359375, 'learning_rate': 1.6833333333333334e-05, 'epoch': 2.0}\n",
      "{'eval_loss': 1.235203742980957, 'eval_precision': 0.4852210657749306, 'eval_recall': 0.50634164864799, 'eval_f1': 0.46792197653856255, 'eval_accuracy': 0.4854111405835544, 'eval_runtime': 5.9511, 'eval_samples_per_second': 63.35, 'eval_steps_per_second': 15.964, 'epoch': 2.0}\n",
      "{'loss': 0.6356, 'grad_norm': 3.5790600776672363, 'learning_rate': 1.2666666666666668e-05, 'epoch': 2.25}\n",
      "{'loss': 0.7612, 'grad_norm': 5.699734687805176, 'learning_rate': 8.500000000000002e-06, 'epoch': 2.5}\n",
      "{'loss': 0.7719, 'grad_norm': 3.529297351837158, 'learning_rate': 4.333333333333334e-06, 'epoch': 2.75}\n",
      "{'loss': 0.7472, 'grad_norm': 12.784101486206055, 'learning_rate': 1.6666666666666668e-07, 'epoch': 3.0}\n",
      "{'eval_loss': 1.2011618614196777, 'eval_precision': 0.49794160549595334, 'eval_recall': 0.4857790755158429, 'eval_f1': 0.4733945582372098, 'eval_accuracy': 0.46684350132625996, 'eval_runtime': 5.4583, 'eval_samples_per_second': 69.069, 'eval_steps_per_second': 17.405, 'epoch': 3.0}\n",
      "{'train_runtime': 93.3026, 'train_samples_per_second': 12.861, 'train_steps_per_second': 3.215, 'train_loss': 1.0397399616241456, 'epoch': 3.0}\n",
      "\n",
      "=== Results for DistilBERT (MIDDLE ONLY) ===\n",
      "=== Overall (Macro Avg) ===\n",
      "Precision: 0.49794160549595334\n",
      "Recall: 0.4857790755158429\n",
      "F1: 0.4733945582372098\n",
      "Accuracy: 0.46684350132625996\n",
      "\n",
      "=== Per Emotion (Class) Metrics ===\n",
      "Angry:\n",
      "  Precision: 0.7083333333333334\n",
      "  Recall:    0.7183098591549296\n",
      "  F1:        0.7132867132867133\n",
      "Happy:\n",
      "  Precision: 0.51\n",
      "  Recall:    0.4811320754716981\n",
      "  F1:        0.49514563106796117\n",
      "Relaxed:\n",
      "  Precision: 0.4318181818181818\n",
      "  Recall:    0.18811881188118812\n",
      "  F1:        0.2620689655172414\n",
      "Sad:\n",
      "  Precision: 0.3416149068322981\n",
      "  Recall:    0.5555555555555556\n",
      "  F1:        0.4230769230769231\n",
      "\n",
      "\n",
      "=== Training DistilBERT (END ONLY) ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 400/400 [00:00<00:00, 12873.66 examples/s]\n",
      "Map: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 377/377 [00:00<00:00, 16475.67 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4134, 'grad_norm': 6.828812599182129, 'learning_rate': 4.600000000000001e-05, 'epoch': 0.25}\n",
      "{'loss': 1.3891, 'grad_norm': 2.9576778411865234, 'learning_rate': 4.183333333333334e-05, 'epoch': 0.5}\n",
      "{'loss': 1.3693, 'grad_norm': 3.1087357997894287, 'learning_rate': 3.766666666666667e-05, 'epoch': 0.75}\n",
      "{'loss': 1.3423, 'grad_norm': 4.609830379486084, 'learning_rate': 3.35e-05, 'epoch': 1.0}\n",
      "{'eval_loss': 1.2294718027114868, 'eval_precision': 0.510072402565609, 'eval_recall': 0.4874881289617136, 'eval_f1': 0.47069412069412075, 'eval_accuracy': 0.47480106100795755, 'eval_runtime': 5.5845, 'eval_samples_per_second': 67.508, 'eval_steps_per_second': 17.011, 'epoch': 1.0}\n",
      "{'loss': 1.0863, 'grad_norm': 4.492818355560303, 'learning_rate': 2.9333333333333336e-05, 'epoch': 1.25}\n",
      "{'loss': 0.9779, 'grad_norm': 7.61391019821167, 'learning_rate': 2.5166666666666667e-05, 'epoch': 1.5}\n",
      "{'loss': 1.132, 'grad_norm': 7.3603057861328125, 'learning_rate': 2.1e-05, 'epoch': 1.75}\n",
      "{'loss': 0.8536, 'grad_norm': 19.257381439208984, 'learning_rate': 1.6833333333333334e-05, 'epoch': 2.0}\n",
      "{'eval_loss': 1.1464648246765137, 'eval_precision': 0.5025612849040154, 'eval_recall': 0.5187724691948334, 'eval_f1': 0.4942400249685226, 'eval_accuracy': 0.506631299734748, 'eval_runtime': 5.3848, 'eval_samples_per_second': 70.012, 'eval_steps_per_second': 17.642, 'epoch': 2.0}\n",
      "{'loss': 0.5894, 'grad_norm': 4.386114120483398, 'learning_rate': 1.2666666666666668e-05, 'epoch': 2.25}\n",
      "{'loss': 0.666, 'grad_norm': 4.350582122802734, 'learning_rate': 8.500000000000002e-06, 'epoch': 2.5}\n",
      "{'loss': 0.6801, 'grad_norm': 6.541337966918945, 'learning_rate': 4.333333333333334e-06, 'epoch': 2.75}\n",
      "{'loss': 0.6543, 'grad_norm': 13.894503593444824, 'learning_rate': 1.6666666666666668e-07, 'epoch': 3.0}\n",
      "{'eval_loss': 1.2069458961486816, 'eval_precision': 0.5021941477385305, 'eval_recall': 0.517385753988633, 'eval_f1': 0.5075828225700492, 'eval_accuracy': 0.5039787798408488, 'eval_runtime': 5.5118, 'eval_samples_per_second': 68.398, 'eval_steps_per_second': 17.236, 'epoch': 3.0}\n",
      "{'train_runtime': 91.1981, 'train_samples_per_second': 13.158, 'train_steps_per_second': 3.29, 'train_loss': 1.0128255939483644, 'epoch': 3.0}\n",
      "\n",
      "=== Results for DistilBERT (END ONLY) ===\n",
      "=== Overall (Macro Avg) ===\n",
      "Precision: 0.5021941477385305\n",
      "Recall: 0.517385753988633\n",
      "F1: 0.5075828225700492\n",
      "Accuracy: 0.5039787798408488\n",
      "\n",
      "=== Per Emotion (Class) Metrics ===\n",
      "Angry:\n",
      "  Precision: 0.5681818181818182\n",
      "  Recall:    0.704225352112676\n",
      "  F1:        0.6289308176100629\n",
      "Happy:\n",
      "  Precision: 0.5825242718446602\n",
      "  Recall:    0.5660377358490566\n",
      "  F1:        0.5741626794258373\n",
      "Relaxed:\n",
      "  Precision: 0.4489795918367347\n",
      "  Recall:    0.43564356435643564\n",
      "  F1:        0.44221105527638194\n",
      "Sad:\n",
      "  Precision: 0.4090909090909091\n",
      "  Recall:    0.36363636363636365\n",
      "  F1:        0.3850267379679144\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_distilbert_segment(train_start_ds, dev_start_ds, label2id, id2label, name=\"BEGINNING ONLY\")\n",
    "\n",
    "train_distilbert_segment(train_middle_ds, dev_middle_ds, label2id, id2label, name=\"MIDDLE ONLY\")\n",
    "\n",
    "train_distilbert_segment(train_end_ds, dev_end_ds, label2id, id2label, name=\"END ONLY\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ae7b19-d2c4-4e49-a965-56855dc7cd29",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
